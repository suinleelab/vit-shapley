{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a263fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/chanwkim/vit-shapley/notebooks\n",
      "/homes/gws/chanwkim/vit-shapley\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f348f8",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548e5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vit_shapley.datamodules.ImageNette_datamodule import ImageNetteDataModule\n",
    "from vit_shapley.datamodules.MURA_datamodule import MURADataModule\n",
    "from vit_shapley.datamodules.Pet_datamodule import PetDataModule\n",
    "\n",
    "from vit_shapley.modules.classifier import Classifier\n",
    "from vit_shapley.modules.classifier_masked import ClassifierMasked\n",
    "from vit_shapley.modules.surrogate import Surrogate\n",
    "from vit_shapley.modules.explainer import Explainer\n",
    "\n",
    "from vit_shapley.config import ex\n",
    "from vit_shapley.config import config, env_chanwkim, dataset_ImageNette, dataset_MURA, dataset_Pet\n",
    "_config=config()\n",
    "\n",
    "dataset_split=\"test\"\n",
    "parallel_mode = (2, 4)\n",
    "backbone_to_use=[\"vit_large_patch16_224\"]\n",
    "_config.update(dataset_ImageNette())\n",
    "evaluation_stage=[\"1_classifier_evaluate\",\n",
    "                  \"2_surrogate_evaluate\",\n",
    "                  \"3_explanation_generate\",\n",
    "                  \"4_insert_delete\",\n",
    "                  \"5_sensitivity\",\n",
    "                  \"6_noretraining\",\n",
    "                  \"7_classifiermasked\",\n",
    "                  \"8_elapsedtime\",\n",
    "                  \"9_estimationerror\"][3]\n",
    "\n",
    "_config.update(env_chanwkim()); _config.update({'gpus_classifier':[3,],\n",
    "                                                'gpus_surrogate':[3,],\n",
    "                                                'gpus_explainer':[3,]})\n",
    "\n",
    "_config.update({'classifier_backbone_type': None,\n",
    "                'classifier_download_weight': False,\n",
    "                'classifier_load_path': None})\n",
    "_config.update({'classifier_masked_mask_location': \"pre-softmax\",\n",
    "                'classifier_enable_pos_embed': True,\n",
    "                })\n",
    "_config.update({'surrogate_mask_location': \"pre-softmax\"})\n",
    "_config.update({'surrogate_backbone_type': None,\n",
    "                'surrogate_download_weight': False,\n",
    "                'surrogate_load_path': None})\n",
    "_config.update({'explainer_num_mask_samples': 2,\n",
    "                'explainer_paired_mask_samples': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37ml0.cs.washington.edu         \u001b[m  Tue Feb 28 17:23:36 2023  \u001b[1m\u001b[30m515.65.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  247\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 79'C\u001b[m, \u001b[1m\u001b[32m 99 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7830\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m7583M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 55'C\u001b[m, \u001b[1m\u001b[32m 98 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 8988\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m8741M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 40'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  247\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 43'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1395\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 49'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3594\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 34'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10788\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m10541M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 51'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7162\u001b[m / \u001b[33m11264\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6058196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _config[\"datasets\"]==\"ImageNette\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/1yndrggu/checkpoints/epoch=14-step=2204.ckpt\",\n",
    "            \"classifier_masked_path\": \"results/wandb_transformer_interpretability_project/fdm70w72/checkpoints/epoch=19-step=2939.ckpt\",\n",
    "            \"surrogate_path\":{\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/3lfv4nmn/checkpoints/epoch=39-step=5879.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/3biv2s85/checkpoints/epoch=60-step=9027.ckpt\"\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/2rq1issn/checkpoints/epoch=16-step=2498.ckpt\",\n",
    "            \"classifier_masked_path\": \"results/wandb_transformer_interpretability_project/x59c992d/checkpoints/epoch=21-step=3233.ckpt\",\n",
    "            \"surrogate_path\":{\n",
    "                #\"original\": \"results/wandb_transformer_interpretability_project/2rq1issn/checkpoints/epoch=16-step=2498.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/3i6zzjnp/checkpoints/epoch=38-step=5732.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/zyybgzcm/checkpoints/epoch=22-step=3380.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1gi5gmrm/checkpoints/epoch=36-step=5438.ckpt\"\n",
    "                },\n",
    "            \"explainer_path\": \"results/wandb_transformer_interpretability_project/3ty85eft/checkpoints/epoch=83-step=12431.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"vit_large_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/1at36lgp/checkpoints/epoch=2-step=440.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                \"pre-softmax\":\"results/wandb_transformer_interpretability_project/284sm0on/checkpoints/epoch=37-step=5585.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/34gbowsg/checkpoints/epoch=91-step=13615.ckpt\"\n",
    "        }\n",
    "    })    \n",
    "elif _config[\"datasets\"]==\"MURA\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\":\"results/wandb_transformer_interpretability_project/1u2xgwks/checkpoints/epoch=15-step=8255.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                #\"original\": \"results/wandb_transformer_interpretability_project/1u2xgwks/checkpoints/epoch=15-step=8255.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/22ompjqu/checkpoints/epoch=47-step=24767.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/2z2qs6t0/checkpoints/epoch=44-step=23219.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1pbmwnvb/checkpoints/epoch=45-step=23735.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/1dmhcwej/checkpoints/epoch=93-step=48597.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        }\n",
    "    })\n",
    "    \n",
    "elif _config[\"datasets\"]==\"Pet\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\":\"results/wandb_transformer_interpretability_project/3g01rci7/checkpoints/epoch=9-step=909.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                \"original\": \"results/wandb_transformer_interpretability_project/3g01rci7/checkpoints/epoch=9-step=909.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/146vf465/checkpoints/epoch=40-step=3730.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/2z2qs6t0/checkpoints/epoch=44-step=23219.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1pbmwnvb/checkpoints/epoch=45-step=23735.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/2oq7lhr7/checkpoints/epoch=85-step=7911.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        }\n",
    "    })    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8426be8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d6d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(num_players: int, num_mask_samples: int or None = None, paired_mask_samples: bool = True,\n",
    "                  mode: str = 'uniform', random_state: np.random.RandomState or None = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_players: the number of players in the coalitional game\n",
    "        num_mask_samples: the number of masks to generate\n",
    "        paired_mask_samples: if True, the generated masks are pairs of x and 1-x.\n",
    "        mode: the distribution that the number of masked features follows. ('uniform' or 'shapley')\n",
    "        random_state: random generator\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape\n",
    "        (num_masks, num_players) if num_masks is int\n",
    "        (num_players) if num_masks is None\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = random_state or np.random\n",
    "\n",
    "    num_samples_ = num_mask_samples or 1\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        assert num_samples_ % 2 == 0, \"'num_samples' must be a multiple of 2 if 'paired' is True\"\n",
    "        num_samples_ = num_samples_ // 2\n",
    "    else:\n",
    "        num_samples_ = num_samples_\n",
    "\n",
    "    if mode == 'uniform':\n",
    "        masks = (random_state.rand(num_samples_, num_players) > random_state.rand(num_samples_, 1)).astype('int')\n",
    "    elif mode == 'shapley':\n",
    "        probs = 1 / (np.arange(1, num_players) * (num_players - np.arange(1, num_players)))\n",
    "        probs = probs / probs.sum()\n",
    "        masks = (random_state.rand(num_samples_, num_players) > 1 / num_players * random_state.choice(\n",
    "            np.arange(num_players - 1), p=probs, size=[num_samples_, 1])).astype('int')\n",
    "    else:\n",
    "        raise ValueError(\"'mode' must be 'random' or 'shapley'\")\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        masks = np.stack([masks, 1 - masks], axis=1).reshape(num_samples_ * 2, num_players)\n",
    "\n",
    "    if num_mask_samples is None:\n",
    "        masks = masks.squeeze(0)\n",
    "        return masks  # (num_masks)\n",
    "    else:\n",
    "        return masks  # (num_samples, num_masks)\n",
    "\n",
    "def set_datamodule(datasets,\n",
    "                   dataset_location,\n",
    "                   explanation_location_train,\n",
    "                   explanation_mask_amount_train,\n",
    "                   explanation_mask_ascending_train,\n",
    "                   \n",
    "                   explanation_location_val,\n",
    "                   explanation_mask_amount_val,\n",
    "                   explanation_mask_ascending_val,                   \n",
    "                   \n",
    "                   explanation_location_test,\n",
    "                   explanation_mask_amount_test,\n",
    "                   explanation_mask_ascending_test,                   \n",
    "                   \n",
    "                   transforms_train,\n",
    "                   transforms_val,\n",
    "                   transforms_test,\n",
    "                   num_workers,\n",
    "                   per_gpu_batch_size,\n",
    "                   test_data_split):\n",
    "    dataset_parameters = {\n",
    "        \"dataset_location\": dataset_location,\n",
    "        \"explanation_location_train\": explanation_location_train,\n",
    "        \"explanation_mask_amount_train\": explanation_mask_amount_train,\n",
    "        \"explanation_mask_ascending_train\": explanation_mask_ascending_train,\n",
    "        \n",
    "        \"explanation_location_val\": explanation_location_val,\n",
    "        \"explanation_mask_amount_val\": explanation_mask_amount_val,\n",
    "        \"explanation_mask_ascending_val\": explanation_mask_ascending_val,\n",
    "        \n",
    "        \"explanation_location_test\": explanation_location_test,\n",
    "        \"explanation_mask_amount_test\": explanation_mask_amount_test,\n",
    "        \"explanation_mask_ascending_test\": explanation_mask_ascending_test,        \n",
    "        \n",
    "        \"transforms_train\": transforms_train,\n",
    "        \"transforms_val\": transforms_val,\n",
    "        \"transforms_test\": transforms_test,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"per_gpu_batch_size\": per_gpu_batch_size,\n",
    "        \"test_data_split\": test_data_split\n",
    "    }\n",
    "\n",
    "    if datasets == \"CheXpert\":\n",
    "        datamodule = CheXpertDataModule(**dataset_parameters)\n",
    "    elif datasets == \"MIMIC\":\n",
    "        datamodule = MIMICDataModule(**dataset_parameters)\n",
    "    elif datasets == \"MURA\":\n",
    "        datamodule = MURADataModule(**dataset_parameters)\n",
    "    elif datasets == \"ImageNette\":\n",
    "        datamodule = ImageNetteDataModule(**dataset_parameters)\n",
    "    else:\n",
    "        ValueError(\"Invalid 'datasets' configuration\")\n",
    "    return datamodule\n",
    "\n",
    "datamodule = set_datamodule(datasets=_config[\"datasets\"],\n",
    "                            dataset_location=_config[\"dataset_location\"],\n",
    "\n",
    "                            explanation_location_train=_config[\"explanation_location_train\"],\n",
    "                            explanation_mask_amount_train=_config[\"explanation_mask_amount_train\"],\n",
    "                            explanation_mask_ascending_train=_config[\"explanation_mask_ascending_train\"],\n",
    "\n",
    "                            explanation_location_val=_config[\"explanation_location_val\"],\n",
    "                            explanation_mask_amount_val=_config[\"explanation_mask_amount_val\"],\n",
    "                            explanation_mask_ascending_val=_config[\"explanation_mask_ascending_val\"],\n",
    "\n",
    "                            explanation_location_test=_config[\"explanation_location_test\"],\n",
    "                            explanation_mask_amount_test=_config[\"explanation_mask_amount_test\"],\n",
    "                            explanation_mask_ascending_test=_config[\"explanation_mask_ascending_test\"],                            \n",
    "\n",
    "                            transforms_train=_config[\"transforms_train\"],\n",
    "                            transforms_val=_config[\"transforms_val\"],\n",
    "                            transforms_test=_config[\"transforms_test\"],\n",
    "                            num_workers=_config[\"num_workers\"],\n",
    "                            per_gpu_batch_size=_config[\"per_gpu_batch_size\"],\n",
    "                            test_data_split=_config[\"test_data_split\"])\n",
    "\n",
    "# The batch for training classifier consists of images and labels, but the batch for training explainer consists of images and masks.\n",
    "# The masks are generated to follow the Shapley distribution.\n",
    "\"\"\"\n",
    "original_getitem = copy.deepcopy(datamodule.dataset_cls.__getitem__)\n",
    "def __getitem__(self, idx):\n",
    "    if self.split == 'train':\n",
    "        masks = generate_mask(num_players=surrogate.num_players,\n",
    "                              num_mask_samples=_config[\"explainer_num_mask_samples\"],\n",
    "                              paired_mask_samples=_config[\"explainer_paired_mask_samples\"], mode='shapley')\n",
    "    elif self.split == 'val' or self.split == 'test':\n",
    "        # get cached if available\n",
    "        if not hasattr(self, \"masks_cached\"):\n",
    "            self.masks_cached = {}\n",
    "        masks = self.masks_cached.setdefault(idx, generate_mask(num_players=surrogate.num_players,\n",
    "                                                                num_mask_samples=_config[\n",
    "                                                                    \"explainer_num_mask_samples\"],\n",
    "                                                                paired_mask_samples=_config[\n",
    "                                                                    \"explainer_paired_mask_samples\"],\n",
    "                                                                mode='shapley'))\n",
    "    else:\n",
    "        raise ValueError(\"'split' variable must be train, val or test.\")\n",
    "    return {\"images\": original_getitem(self, idx)[\"images\"],\n",
    "            \"labels\": original_getitem(self, idx)[\"labels\"],\n",
    "            \"masks\": masks}\n",
    "datamodule.dataset_cls.__getitem__ = __getitem__\n",
    "\"\"\"\n",
    "\n",
    "datamodule.set_train_dataset()\n",
    "datamodule.set_val_dataset()\n",
    "datamodule.set_test_dataset()\n",
    "\n",
    "train_dataset=datamodule.train_dataset\n",
    "val_dataset=datamodule.val_dataset\n",
    "test_dataset=datamodule.test_dataset\n",
    "\n",
    "dset=test_dataset\n",
    "\n",
    "if dataset_split==\"train\":\n",
    "    dset.data = train_dataset.data\n",
    "elif dataset_split==\"val\":\n",
    "    dset.data = val_dataset.data     \n",
    "elif dataset_split==\"test\": \n",
    "    dset.data = test_dataset.data\n",
    "else:\n",
    "    raise\n",
    "\n",
    "labels = np.array([i['label'] for i in dset.data])\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "images_idx_list = [np.where(labels == category)[0] for category in range(num_classes)]\n",
    "\n",
    "images_idx=[]\n",
    "for classidx in range(4,4+int(10/len(images_idx_list))):\n",
    "    images_idx+=[category_idx[classidx] for category_idx in images_idx_list]\n",
    "\n",
    "xy=[dset[idx] for idx in images_idx]\n",
    "x, y = zip(*[(i['images'], i['labels']) for i in xy])\n",
    "x = torch.stack(x)\n",
    "y_labels=[dset.labels[i] for i in y]\n",
    "\n",
    "\n",
    "if _config[\"datasets\"]==\"ImageNette\":\n",
    "    label_name_list=['Cassette player', \n",
    "                      'Garbage truck', \n",
    "                      'Tench', \n",
    "                      'English springer', \n",
    "                      'Church', \n",
    "                      'Parachute', \n",
    "                      'French horn', \n",
    "                      'Chain saw', \n",
    "                      'Golf ball', \n",
    "                      'Gas pump']\n",
    "    \n",
    "elif _config[\"datasets\"]==\"MURA\":\n",
    "    label_name_list=[\"Normal\", \"Abnormal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380660e",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc25f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_large_patch16_224\n"
     ]
    }
   ],
   "source": [
    "backbone_type_config_dict = OrderedDict()\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict_.items()):\n",
    "    if backbone_type in backbone_to_use:\n",
    "        print(backbone_type)\n",
    "        backbone_type_config_dict[backbone_type]=backbone_type_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13aafbd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_dict[backbone_type] = Classifier(backbone_type=backbone_type,\n",
    "                                               download_weight=_config['classifier_download_weight'],\n",
    "                                               load_path=backbone_type_config[\"classifier_path\"],\n",
    "                                               target_type=_config[\"target_type\"],\n",
    "                                               output_dim=_config[\"output_dim\"],\n",
    "                                               enable_pos_embed=_config[\"classifier_enable_pos_embed\"],\n",
    "\n",
    "                                               checkpoint_metric=None,\n",
    "                                               loss_weight=None,\n",
    "                                               optim_type=None,\n",
    "                                               learning_rate=None,\n",
    "                                               weight_decay=None,\n",
    "                                               decay_power=None,\n",
    "                                               warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3016a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict_ = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_dict_[backbone_type] = Surrogate(mask_location=_config[\"surrogate_mask_location\"],\n",
    "                                                   backbone_type=backbone_type,\n",
    "                                                   download_weight=_config['classifier_download_weight'],\n",
    "                                                   load_path=backbone_type_config[\"classifier_path\"],\n",
    "                                                   target_type=_config[\"target_type\"],\n",
    "                                                   output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                                   target_model=None,\n",
    "                                                   checkpoint_metric=None,\n",
    "                                                   optim_type=None,\n",
    "                                                   learning_rate=None,\n",
    "                                                   weight_decay=None,\n",
    "                                                   decay_power=None,\n",
    "                                                   warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"7_classifiermasked\":\n",
    "    classifier_masked_dict = OrderedDict()\n",
    "\n",
    "    for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_masked_dict[backbone_type] = ClassifierMasked(mask_location=_config[\"classifier_masked_mask_location\"],\n",
    "                                                               backbone_type=backbone_type,\n",
    "                                                               download_weight=_config['classifier_download_weight'],\n",
    "                                                               load_path=backbone_type_config[\"classifier_masked_path\"],\n",
    "                                                               target_type=_config[\"target_type\"],\n",
    "                                                               output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                                               checkpoint_metric=None,\n",
    "                                                               loss_weight=None,                                                             \n",
    "                                                               optim_type=None,\n",
    "                                                               learning_rate=None,\n",
    "                                                               weight_decay=None,\n",
    "                                                               decay_power=None,\n",
    "                                                               warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a294aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    mask_method_dict = OrderedDict()\n",
    "    for mask_location in backbone_type_config[\"surrogate_path\"].keys():\n",
    "        mask_method_dict[mask_location] = Surrogate(mask_location=mask_location if mask_location!=\"original\" else \"pre-softmax\",\n",
    "                                          backbone_type=backbone_type,\n",
    "                                          download_weight=_config['surrogate_download_weight'],\n",
    "                                          load_path=backbone_type_config[\"surrogate_path\"][mask_location],\n",
    "                                          target_type=_config[\"target_type\"],\n",
    "                                          output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                          target_model=None,\n",
    "                                          checkpoint_metric=None,\n",
    "                                          optim_type=None,\n",
    "                                          learning_rate=None,\n",
    "                                          weight_decay=None,\n",
    "                                          decay_power=None,\n",
    "                                          warmup_steps=None).to(_config[\"gpus_surrogate\"][idx])\n",
    "    surrogate_dict[backbone_type]=mask_method_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33775c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a2343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8414a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vitmedical.modules.explainer import Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c7ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276fe1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_config.update({'explainer_normalization': \"additive\",\n",
    "                'explainer_activation': \"tanh\",\n",
    "                'explainer_link': 'sigmoid' if _config[\"output_dim\"]==1 else 'softmax',\n",
    "                'explainer_head_num_attention_blocks': 1,\n",
    "                'explainer_head_include_cls': True,\n",
    "                'explainer_head_num_mlp_layers': 3,\n",
    "                'explainer_head_mlp_layer_ratio': 4,\n",
    "                'explainer_residual': [],\n",
    "                'explainer_freeze_backbone': \"all\"})\n",
    "\n",
    "explainer_dict = OrderedDict()\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    explainer_dict[backbone_type] = Explainer(normalization=_config[\"explainer_normalization\"],\n",
    "                                              normalization_class=_config[\"explainer_normalization_class\"],\n",
    "                                              activation=_config[\"explainer_activation\"],\n",
    "                                              surrogate=surrogate_dict[backbone_type][\"pre-softmax\"],\n",
    "                                              link=_config[\"explainer_link\"],\n",
    "                                              backbone_type=backbone_type,\n",
    "                                              download_weight=False,\n",
    "                                              residual=_config['explainer_residual'],\n",
    "                                              load_path=backbone_type_config[\"explainer_path\"],\n",
    "                                              target_type=_config[\"target_type\"],\n",
    "                                              output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                              explainer_head_num_attention_blocks=_config[\"explainer_head_num_attention_blocks\"],\n",
    "                                              explainer_head_include_cls=_config[\"explainer_head_include_cls\"],\n",
    "                                              explainer_head_num_mlp_layers=_config[\"explainer_head_num_mlp_layers\"],\n",
    "                                              explainer_head_mlp_layer_ratio=_config[\"explainer_head_mlp_layer_ratio\"],\n",
    "                                              explainer_norm=_config[\"explainer_norm\"],\n",
    "\n",
    "                                              efficiency_lambda=_config[\"explainer_efficiency_lambda\"],\n",
    "                                              efficiency_class_lambda=_config[\"explainer_efficiency_class_lambda\"],\n",
    "                                              freeze_backbone=_config[\"explainer_freeze_backbone\"],\n",
    "\n",
    "                                              checkpoint_metric=_config[\"checkpoint_metric\"],\n",
    "                                              optim_type=_config[\"optim_type\"],\n",
    "                                              learning_rate=_config[\"learning_rate\"],\n",
    "                                              weight_decay=_config[\"weight_decay\"],\n",
    "                                              decay_power=_config[\"decay_power\"],\n",
    "                                              warmup_steps=_config[\"warmup_steps\"]).to(_config[\"gpus_explainer\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7f6fb",
   "metadata": {},
   "source": [
    "# explanation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02048ea8",
   "metadata": {},
   "source": [
    "## attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_attention(attentions, add_residual=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions: (num_batches, num_layers, num_players, num_players)\n",
    "        add_residual: bool\n",
    "    Returns:\n",
    "        joint_attentions: (num_batches, num_layers, num_players, num_players)\n",
    "    \"\"\"\n",
    "    assert len(attentions.shape)==4\n",
    "    if add_residual:\n",
    "        residual_att = np.eye(attentions.shape[2])[np.newaxis, np.newaxis, ...]\n",
    "        aug_attentions = attentions + residual_att\n",
    "        aug_attentions = aug_attentions / aug_attentions.sum(axis=-1)[..., np.newaxis]\n",
    "    else:\n",
    "        aug_attentions =  attentions\n",
    "    \n",
    "    joint_attentions = np.zeros(aug_attentions.shape) # (num_batches, num_layers, num_players, num_players)\n",
    "\n",
    "    for i in np.arange(joint_attentions.shape[1]):\n",
    "        if i==0:\n",
    "            joint_attentions[:,i] = aug_attentions[:,0]\n",
    "        else:\n",
    "            joint_attentions[:,i] = (aug_attentions[:,i] @ joint_attentions[:,i-1])\n",
    "    return joint_attentions\n",
    "\n",
    "\n",
    "def attentions_to_explanation(attentions, mode='rollout'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions: (num_batches, num_layers, num_heads, num_players, num_players)\n",
    "    \"\"\"\n",
    "    assert len(attentions.shape)==5 and attentions.shape[-1]==attentions.shape[-2]\n",
    "    attentions_nohead = attentions.sum(axis=2)/attentions.shape[2] # (num_batch, num_layers, num_players, num_players)\n",
    "    attentions_nohead_residual = attentions_nohead + np.eye(attentions_nohead.shape[2])[np.newaxis, np.newaxis, ...] # (num_batch, num_layers, num_players, num_players)\n",
    "    attentions_nohead_residual_normalized = attentions_nohead_residual / attentions_nohead_residual.sum(axis=-1)[..., np.newaxis] # (num_batch, num_layers, num_players, num_players)\n",
    "    \n",
    "    if isinstance(mode, int):\n",
    "        return attentions_nohead_residual_normalized[:, mode, 0, 1:]\n",
    "    elif mode=='raw':\n",
    "        return attentions_nohead_residual_normalized[:, -1, 0, 1:]\n",
    "    elif mode=='rollout':\n",
    "        attentions_nohead_residual_normalized_rollout = compute_joint_attention(attentions_nohead_residual_normalized,\n",
    "                                                                                add_residual=False)\n",
    "        return attentions_nohead_residual_normalized_rollout[:, -1, 0, 1:]\n",
    "#explanation_to_mask(attention_rollout).argmin(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f301b",
   "metadata": {},
   "source": [
    "## lrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8045007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.transformer_explainability.baselines.ViT.ViT_new as ViT_new\n",
    "import utils.transformer_explainability.baselines.ViT.ViT_LRP as ViT_LRP\n",
    "import utils.transformer_explainability.baselines.ViT.ViT_orig_LRP as ViT_orig_LRP\n",
    "\n",
    "from utils.transformer_explainability.baselines.ViT.ViT_explanation_generator import Baselines, LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055b095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baselines_dict = OrderedDict()\n",
    "lrp_dict = OrderedDict()\n",
    "orig_lrp_dict = OrderedDict()\n",
    "\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    checkpoint = torch.load(backbone_type_config[\"classifier_path\"], map_location=\"cpu\")\n",
    "    checkpoint[\"state_dict\"]=OrderedDict([(k.replace('backbone.',''), v) for k, v in checkpoint[\"state_dict\"].items()])\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    \n",
    "    model = getattr(ViT_new, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "    ret = model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "    print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "    print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output1=model(x.to(next(model.parameters()).device))\n",
    "        output2=classifier_dict[backbone_type](x.to(next(model.parameters()).device))['logits']\n",
    "        assert torch.allclose(output1,output2,atol=1e-03)\n",
    "    baselines = Baselines(model)\n",
    "    baselines_dict[backbone_type]=baselines        \n",
    "    \n",
    "    model_LRP=getattr(ViT_LRP, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "    ret = model_LRP.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "    print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "    print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "    model_LRP.eval()      \n",
    "    lrp = LRP(model_LRP)\n",
    "    lrp_dict[backbone_type]=lrp\n",
    "    \n",
    "#     model_orig_LRP=getattr(ViT_orig_LRP, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "#     ret = model_orig_LRP.load_state_dict(state_dict, strict=False)\n",
    "#     print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "#     print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "#     print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "#     model_orig_LRP.eval()    \n",
    "#     orig_lrp = LRP(model_orig_LRP)  \n",
    "#     orig_lrp_dict[backbone_type]=orig_lrp\n",
    "    \n",
    "    \n",
    "def get_lrp_module_explanation(backbone_type, original_image, class_index=None, mode='transformer_attribution'):\n",
    "    if mode==\"transformer_attribution\": # ours\n",
    "        transformer_attribution = lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(lrp_dict[backbone_type].model.parameters()).device), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    elif mode==\"rollout\": # rollout\n",
    "        transformer_attribution = baselines_dict[backbone_type].generate_rollout(original_image.unsqueeze(0).to(next(baselines_dict[backbone_type].model.parameters()).device), start_layer=1).detach()\n",
    "    elif mode==\"attn_last_layer\": # raw-attention\n",
    "        transformer_attribution = lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(lrp_dict[backbone_type].model.parameters()).device), method=\"last_layer_attn\", index=class_index).detach()\n",
    "    elif mode == 'attn_gradcam': # GradCAM\n",
    "        transformer_attribution = baselines_dict[backbone_type].generate_cam_attn(original_image.unsqueeze(0).to(next(baselines_dict[backbone_type].model.parameters()).device), index=class_index).detach()\n",
    "        transformer_attribution = transformer_attribution.reshape(1,-1)\n",
    "        #transformer_attribution=torch.nan_to_num(transformer_attribution,nan=0)\n",
    "        #transformer_attribution+=torch.rand(size=transformer_attribution.shape, device=transformer_attribution.device)*1e-20        \n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    elif mode == 'full_lrp':\n",
    "        transformer_attribution = orig_lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(orig_lrp_dict[backbone_type].model.parameters()).device), method=\"full\", index=class_index).detach()\n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    elif mode == 'lrp_last_layer':\n",
    "        transformer_attribution = orig_lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(orig_lrp_dict[backbone_type].model.parameters()).device), method=\"last_layer\", index=class_index).detach()\n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    #print(transformer_attribution.max(), transformer_attribution.min())\n",
    "    #print(transformer_attribution.shape)\n",
    "    return transformer_attribution    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40064fa3",
   "metadata": {},
   "source": [
    "## CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch_grad_cam import GradCAM\n",
    "from utils.pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n",
    "        height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "class WrapperLogits(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model(images)\n",
    "        return x['logits']\n",
    "\n",
    "cam_dict = OrderedDict()\n",
    "for backbone_type, backbone_type_config in backbone_type_config_dict.items():\n",
    "    cam_dict[backbone_type] = GradCAM(model=WrapperLogits(classifier_dict[backbone_type]),\n",
    "                                      target_layers=[classifier_dict[backbone_type].backbone.blocks[-1].norm1],\n",
    "                                      reshape_transform=reshape_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2b912",
   "metadata": {},
   "source": [
    "## Gradient-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, InputXGradient, Saliency, NoiseTunnel\n",
    "import torch.nn as nn\n",
    "\n",
    "class FromPixel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model.backbone.patch_embed(images)\n",
    "        x = self.model.backbone.forward_features(x)['x']\n",
    "        logits = self.model.head(x)\n",
    "        \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            return logits.sigmoid()\n",
    "        else:\n",
    "            return logits.softmax(dim=-1)\n",
    "    \n",
    "class FromEmbedding(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        x = self.model.backbone.forward_features(embedding)['x']\n",
    "        logits = self.model.head(x)\n",
    "        \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            return logits.sigmoid()\n",
    "        else:\n",
    "            return logits.softmax(dim=-1)\n",
    "\n",
    "#Classifier Wrapping    \n",
    "classifier_pixel_dict = OrderedDict()\n",
    "classifier_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_pixel_dict[backbone_type]=FromPixel(classifier_dict_[backbone_type])\n",
    "    classifier_embedding_dict[backbone_type]=FromEmbedding(classifier_dict_[backbone_type])\n",
    "\n",
    "#Vanilla\n",
    "saliency_pixel_dict = OrderedDict()\n",
    "saliency_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    saliency_pixel_dict[backbone_type] = Saliency(classifier_pixel_dict[backbone_type])\n",
    "    saliency_embedding_dict[backbone_type] = Saliency(classifier_embedding_dict[backbone_type])      \n",
    "\n",
    "#NoiseTunnel\n",
    "noisetunnel_pixel_dict = OrderedDict()\n",
    "noisetunnel_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    noisetunnel_pixel_dict[backbone_type] = NoiseTunnel(saliency_pixel_dict[backbone_type])\n",
    "    noisetunnel_embedding_dict[backbone_type] = NoiseTunnel(saliency_embedding_dict[backbone_type])      \n",
    "\n",
    "#IntegratedGradients    \n",
    "ig_pixel_dict = OrderedDict()\n",
    "ig_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    ig_pixel_dict[backbone_type] = IntegratedGradients(classifier_pixel_dict[backbone_type])\n",
    "    ig_embedding_dict[backbone_type] = IntegratedGradients(classifier_embedding_dict[backbone_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributions_pixel_process(attributions_pixel):\n",
    "    attributions_pixel_sum = attributions_pixel.sum(axis=-3)\n",
    "    attributions_pixel_abssum = attributions_pixel.abs().sum(axis=-3)\n",
    "    attributions_pixel_patchsum = F.conv2d(attributions_pixel,\n",
    "                                           weight=torch.ones(size=(1, 3, 16, 16),\n",
    "                                                             dtype=attributions_pixel.dtype,\n",
    "                                                             device=attributions_pixel.device),\n",
    "                                           stride=16).squeeze(axis=1)#.flatten(1, 2)\n",
    "    attributions_pixel_pathabssum = F.conv2d(attributions_pixel.abs(),\n",
    "                                             weight=torch.ones(size=(1, 3, 16, 16),\n",
    "                                                               dtype=attributions_pixel.dtype,\n",
    "                                                               device=attributions_pixel.device),\n",
    "                                             stride=16).squeeze(axis=1)#.flatten(1, 2) \n",
    "    \n",
    "    return {'attributions_pixel_sum': attributions_pixel_sum.detach().cpu(),# makes sense? (but cannot used for benchmarking)\n",
    "            'attributions_pixel_abssum': attributions_pixel_abssum.detach().cpu(),# makes sense (but cannot used for benchmarking)\n",
    "            'attributions_pixel_patchsum': attributions_pixel_patchsum.detach().cpu(),  # makes sense?\n",
    "            'attributions_pixel_patchabssum': attributions_pixel_pathabssum.detach().cpu()  # makes sense    \n",
    "           }\n",
    "    \n",
    "    \n",
    "def attributions_embedding_process(attributions_embedding):\n",
    "    attributions_embedding_sum = attributions_embedding.sum(axis=-1)\n",
    "    attributions_embedding_abssum = attributions_embedding.abs().sum(axis=-1)\n",
    "    return {'attributions_embedding_sum': attributions_embedding_sum.detach().cpu(), # makes sense?\n",
    "            'attributions_embedding_abssum': attributions_embedding_abssum.detach().cpu() # makes sense\n",
    "           }  \n",
    "\n",
    "def get_vanilla(image, saliency_pixel=None, saliency_embedding=None):\n",
    "    result={}\n",
    "    with torch.no_grad():\n",
    "        if saliency_pixel is not None:\n",
    "            attributions_pixel = [saliency_pixel.attribute(inputs=image.unsqueeze(0).to(next(saliency_pixel.forward_func.parameters()).device), \n",
    "                                                           target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))\n",
    "            \n",
    "        if saliency_embedding is not None:\n",
    "            attributions_embedding = [saliency_embedding.attribute(inputs=saliency_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(saliency_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                   target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_sg(image, noisetunnel_pixel=None, noisetunnel_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if noisetunnel_pixel is not None:\n",
    "            attributions_pixel = [noisetunnel_pixel.attribute(inputs=image.unsqueeze(0).to(next(noisetunnel_pixel.forward_func.parameters()).device),\n",
    "                                                              nt_type='smoothgrad',\n",
    "                                                              nt_samples=10,\n",
    "                                                              target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))\n",
    "\n",
    "        if noisetunnel_embedding is not None:\n",
    "            attributions_embedding  = [noisetunnel_embedding.attribute(inputs=noisetunnel_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(noisetunnel_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                       nt_type='smoothgrad',\n",
    "                                                                       nt_samples=10,            \n",
    "                                                                       target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_vargrad(image, noisetunnel_pixel=None, noisetunnel_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if noisetunnel_pixel is not None:\n",
    "            attributions_pixel = [noisetunnel_pixel.attribute(inputs=image.unsqueeze(0).to(next(noisetunnel_pixel.forward_func.parameters()).device),\n",
    "                                                              nt_type='vargrad',\n",
    "                                                              nt_samples=10,\n",
    "                                                              target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))   \n",
    "\n",
    "        if noisetunnel_embedding is not None:\n",
    "            attributions_embedding  = [noisetunnel_embedding.attribute(inputs=noisetunnel_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(noisetunnel_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                       nt_type='vargrad',\n",
    "                                                                       nt_samples=10,            \n",
    "                                                                       target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))            \n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_ig(image, ig_pixel=None, ig_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if ig_pixel is not None:\n",
    "            attributions_pixel = [ig_pixel.attribute(inputs=image.unsqueeze(0).to(next(ig_pixel.forward_func.parameters()).device),\n",
    "                                                                            baselines=torch.zeros_like(image).unsqueeze(0).to(next(ig_pixel.forward_func.parameters()).device),\n",
    "                                                                            target=i,\n",
    "                                                                            n_steps=10) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))           \n",
    "\n",
    "        if ig_embedding is not None:\n",
    "            attributions_embedding = [ig_embedding.attribute(inputs=ig_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(ig_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                                           baselines=ig_embedding.forward_func.model.backbone.patch_embed(torch.zeros_like(image).unsqueeze(0).to(next(ig_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                                           target=i,\n",
    "                                                                                           n_steps=10) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))          \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d0766",
   "metadata": {},
   "source": [
    "## leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232428ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(image, surrogate=None, classifier=None):\n",
    "    with torch.no_grad():\n",
    "        mask=torch.cat([torch.ones(1, 196) ,1-torch.eye(196)])\n",
    "        if surrogate is not None:\n",
    "            out=surrogate(image.unsqueeze(0).repeat(196+1, 1, 1, 1).to(surrogate.device), \n",
    "                          masks=mask.to(surrogate.device))\n",
    "        elif classifier is not None:\n",
    "            mask_scaled = torch.repeat_interleave(torch.repeat_interleave(mask.reshape(-1, 14, 14), 16, dim=2), 16, dim=1)\n",
    "            image_masked=image * mask_scaled.unsqueeze(1)\n",
    "            \n",
    "            if classifier.__class__==Classifier:\n",
    "                out=classifier(image_masked.to(classifier.device))\n",
    "            elif classifier.__class__==Surrogate:\n",
    "                out=classifier(image_masked.to(classifier.device),\n",
    "                              masks=torch.ones((len(image_masked),196)))\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            prob=out['logits'].sigmoid().detach().cpu().numpy()\n",
    "        else:\n",
    "            prob=out['logits'].softmax(dim=-1).detach().cpu().numpy()    \n",
    "        \n",
    "        result=prob[0:1]-prob[1:]\n",
    "\n",
    "    return result.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef518bd",
   "metadata": {},
   "source": [
    "# RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rise(image, surrogate=None, classifier=None, include_prob=0.5, N=2000):\n",
    "    assert (surrogate is None) != (classifier is None)\n",
    "    \n",
    "    prob_list=[]\n",
    "    mask_list=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(N//100):\n",
    "            mask=torch.rand(100, 196)<include_prob\n",
    "            if surrogate is not None:\n",
    "                out=surrogate(image.unsqueeze(0).repeat(100, 1, 1, 1).to(surrogate.device), \n",
    "                              masks=(mask).to(surrogate.device))\n",
    "            elif classifier is not None:\n",
    "                mask_scaled = torch.repeat_interleave(torch.repeat_interleave(mask.reshape(-1, 14, 14), 16, dim=2), 16, dim=1)\n",
    "                image_masked = image * mask_scaled.unsqueeze(1)\n",
    "                del mask_scaled\n",
    "                if classifier.__class__==Classifier:\n",
    "                    out=classifier(image_masked.to(classifier.device))\n",
    "                elif classifier.__class__==Surrogate:\n",
    "                    out=classifier(image_masked.to(classifier.device),\n",
    "                                  masks=torch.ones_like(mask))\n",
    "                else:\n",
    "                    raise\n",
    "                #out=surrogate_dict[backbone_type](image_masked.to(surrogate_dict[backbone_type].device), \n",
    "                #             masks=torch.ones((100,196)).to(surrogate_dict[backbone_type].device))                \n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "            if _config[\"output_dim\"]==1:\n",
    "                prob=out['logits'].sigmoid().detach().cpu().numpy()\n",
    "            else:\n",
    "                prob=out['logits'].softmax(dim=-1).detach().cpu().numpy()    \n",
    "            \n",
    "            del out\n",
    "            prob_list.append(prob)\n",
    "            mask_list.append(mask.numpy())\n",
    "            del mask\n",
    "            \n",
    "            \n",
    "    prob_list_array=np.concatenate(prob_list) # (num_trials, num_classes)\n",
    "    mask_list_array=np.concatenate(mask_list) # (num_trials, num_players)\n",
    "\n",
    "    result = (prob_list_array.T @ mask_list_array) # (num_classes, num_players)\n",
    "    result = result/mask_list_array.sum(axis=0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1e499",
   "metadata": {},
   "source": [
    "# KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.shapreg import removal, games, shapley\n",
    "\n",
    "class SurrogateSHAPWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            self.activation=nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation=nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        images, mask = x\n",
    "        mask = mask.squeeze(1).flatten(1)\n",
    "        out=self.model(images, mask)['logits']\n",
    "        out=self.activation(out)\n",
    "        return out\n",
    "\n",
    "surrogate_SHAP_wrapped_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    surrogate_SHAP_wrapped_dict[backbone_type]=SurrogateSHAPWrapper(surrogate_dict[backbone_type][\"pre-softmax\"])    \n",
    "\n",
    "def get_shap(surrogate_SHAP_wrapped, x, batch_size=64, thresh=0.2, variance_batches=60):\n",
    "    game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped, x)\n",
    "    explanation = shapley.ShapleyRegression(game, batch_size=batch_size, thresh=thresh, variance_batches=variance_batches)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4f5f",
   "metadata": {},
   "source": [
    "# save_dict_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9cee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    explanation_save_dict_backbone={\"random\":{},\n",
    "                                    \"attention_rollout\":{},\n",
    "                                    \"attention_last\":{},\n",
    "                                    \"LRP\":{},\n",
    "                                    \"gradcam\":{},\n",
    "                                    \"gradcamgithub\": {},\n",
    "                                    \"vanillapixel\": {},\n",
    "                                    \"vanillaembedding\": {},\n",
    "                                    \"sgpixel\": {},\n",
    "                                    \"sgembedding\": {},\n",
    "                                    \"vargradpixel\": {},\n",
    "                                    \"vargradembedding\": {},               \n",
    "                                    \"igpixel\": {},\n",
    "                                    \"igembedding\": {},\n",
    "                                    \"leaveoneoutclassifier\": {},\n",
    "                                    \"leaveoneoutsurrogate\": {},\n",
    "                                    \"riseclassifier\": {},\n",
    "                                    \"risesurrogate\": {},\n",
    "                                    \"ours\": {},\n",
    "                                    \"kernelshap\": {}\n",
    "                                    }\n",
    "    explanation_save_dict[backbone_type]=explanation_save_dict_backbone\n",
    "    \n",
    "def explanation_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, explanation_list, elapsed_time_list, \n",
    "                                 shape=None):\n",
    "    explanation_save_dict_backbone_method=explanation_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(explanation_list) == len(elapsed_time_list)\n",
    "    \n",
    "    for explanation, path, elapsed_time in zip(explanation_list, path_list, elapsed_time_list):\n",
    "        assert type(explanation)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        assert type(elapsed_time)==float\n",
    "        if shape is not None:\n",
    "            assert explanation.shape==shape\n",
    "        explanation_save_dict_backbone_method[path]={\"explanation\": explanation.astype(float),\n",
    "                                                     \"elapsed_time\": elapsed_time}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d38051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +    924   ->    924\n",
      "attention_rollout              0   +    924   ->    924\n",
      "attention_last                 0   +    923   ->    923\n",
      "LRP                            0   +    924   ->    924\n",
      "gradcam                        0   +    924   ->    924\n",
      "gradcamgithub                  0   +    924   ->    924\n",
      "vanillapixel                   0   +    924   ->    924\n",
      "vanillaembedding               0   +    924   ->    924\n",
      "sgpixel                        0   +    924   ->    924\n",
      "sgembedding                    0   +    924   ->    924\n",
      "vargradpixel                   0   +    924   ->    924\n",
      "vargradembedding               0   +    924   ->    924\n",
      "igpixel                        0   +    924   ->    924\n",
      "igembedding                    0   +    924   ->    924\n",
      "leaveoneoutclassifier          0   +    924   ->    924\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    924   ->    924\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    924   ->    924\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "        try:\n",
    "            explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(explanation_save_dict_path):\n",
    "                with open(explanation_save_dict_path, 'rb') as f:\n",
    "                    explanation_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                explanation_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(explanation_save_dict_backbone_method)            \n",
    "            len_loaded=len(explanation_save_dict_loaded)\n",
    "            explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "            len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}') \n",
    "        except:\n",
    "            print('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a028c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertdelete_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    insertdelete_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                     \"kernelshap\":{}\n",
    "                                    }\n",
    "    insertdelete_save_dict[backbone_type]=insertdelete_save_dict_backbone\n",
    "    \n",
    "def insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    insertdelete_save_dict_backbone_method=insertdelete_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        insertdelete_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34817dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +    677   ->    677\n",
      "attention_rollout              0   +    677   ->    677\n",
      "attention_last                 0   +    677   ->    677\n",
      "LRP                            0   +    677   ->    677\n",
      "gradcam                        0   +    677   ->    677\n",
      "gradcamgithub                  0   +    677   ->    677\n",
      "vanillapixel                   0   +    677   ->    677\n",
      "vanillaembedding               0   +    677   ->    677\n",
      "sgpixel                        0   +    677   ->    677\n",
      "sgembedding                    0   +    677   ->    677\n",
      "vargradpixel                   0   +    677   ->    677\n",
      "vargradembedding               0   +    677   ->    677\n",
      "igpixel                        0   +    677   ->    677\n",
      "igembedding                    0   +    677   ->    677\n",
      "leaveoneoutclassifier          0   +    677   ->    677\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    677   ->    677\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    677   ->    677\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "        insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(insertdelete_save_dict_path):\n",
    "            with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                insertdelete_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            insertdelete_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "        len_loaded=len(insertdelete_save_dict_loaded)\n",
    "        insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "        len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a38937",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_save_dit={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    sensitivity_save_dit_backbone={\"attention_rollout\":{},\n",
    "                                   \"attention_last\":{},\n",
    "                                   \"LRP\":{},\n",
    "                                   \"gradcam\":{},\n",
    "                                   \"gradcamgithub\": {},\n",
    "                                   \"vanillapixel\": {},\n",
    "                                   \"vanillaembedding\": {},\n",
    "                                   \"sgpixel\": {},\n",
    "                                   \"sgembedding\": {},\n",
    "                                   \"vargradpixel\": {},\n",
    "                                   \"vargradembedding\": {},               \n",
    "                                   \"igpixel\": {},\n",
    "                                   \"igembedding\": {},\n",
    "                                   \"leaveoneoutclassifier\": {},\n",
    "                                   \"leaveoneoutsurrogate\": {},\n",
    "                                   \"riseclassifier\": {},\n",
    "                                   \"risesurrogate\": {},\n",
    "                                   \"ours\": {},\n",
    "                                   }\n",
    "    sensitivity_save_dit[backbone_type]=sensitivity_save_dit_backbone\n",
    "    \n",
    "def sensitivity_save_dit_update(backbone_type, explanation_method, num_included_players,\n",
    "                                path_list, sensitivity_list,\n",
    "                                shape=None):\n",
    "    \n",
    "    sensitivity_save_dit_backbone_method=sensitivity_save_dit[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(sensitivity_list)\n",
    "    \n",
    "    for sensitivity, path in zip(sensitivity_list, path_list):\n",
    "        assert type(sensitivity)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert sensitivity.shape==shape\n",
    "        sensitivity_save_dit_backbone_method.setdefault(path, {})\n",
    "        sensitivity_save_dit_backbone_method[path][num_included_players]=sensitivity.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df2a81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout              0   +    816   ->    816\n",
      "attention_last                 0   +    816   ->    816\n",
      "LRP                            0   +    816   ->    816\n",
      "gradcam                        0   +    816   ->    816\n",
      "gradcamgithub                  0   +    816   ->    816\n",
      "vanillapixel                   0   +    816   ->    816\n",
      "vanillaembedding               0   +    816   ->    816\n",
      "sgpixel                        0   +    816   ->    816\n",
      "sgembedding                    0   +    816   ->    816\n",
      "vargradpixel                   0   +    816   ->    816\n",
      "vargradembedding               0   +    815   ->    815\n",
      "igpixel                        0   +    815   ->    815\n",
      "igembedding                    0   +    815   ->    815\n",
      "leaveoneoutclassifier          0   +    815   ->    815\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    815   ->    815\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    814   ->    814\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, sensitivity_save_dit_backbone_method in sensitivity_save_dit[backbone_type].items():\n",
    "        sensitivity_save_dit_path=f'results/5_sensitivity/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(sensitivity_save_dit_path):\n",
    "            with open(sensitivity_save_dit_path, 'rb') as f:\n",
    "                sensitivity_save_dit_loaded=pickle.load(f)\n",
    "        else:\n",
    "            sensitivity_save_dit_loaded={}\n",
    "\n",
    "        len_original=len(sensitivity_save_dit_backbone_method)            \n",
    "        len_loaded=len(sensitivity_save_dit_loaded)\n",
    "        sensitivity_save_dit_backbone_method.update(sensitivity_save_dit_loaded)\n",
    "        len_updated=len(sensitivity_save_dit_backbone_method)\n",
    "\n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c313913",
   "metadata": {},
   "outputs": [],
   "source": [
    "noretraining_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    noretraining_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    noretraining_save_dict[backbone_type]=noretraining_save_dict_backbone\n",
    "    \n",
    "def noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    noretraining_save_dict_backbone_method=noretraining_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        noretraining_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644df8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, noretraining_save_dict_backbone_method in noretraining_save_dict[backbone_type].items():\n",
    "        noretraining_save_dict_path=f'results/6_noretraining/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(noretraining_save_dict_path):\n",
    "            with open(noretraining_save_dict_path, 'rb') as f:\n",
    "                noretraining_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            noretraining_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(noretraining_save_dict_backbone_method)            \n",
    "        len_loaded=len(noretraining_save_dict_loaded)\n",
    "        noretraining_save_dict_backbone_method.update(noretraining_save_dict_loaded)\n",
    "        len_updated=len(noretraining_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiermasked_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifiermasked_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    classifiermasked_save_dict[backbone_type]=classifiermasked_save_dict_backbone\n",
    "    \n",
    "def classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    classifiermasked_save_dict_backbone_method=classifiermasked_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        classifiermasked_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, classifiermasked_save_dict_backbone_method in classifiermasked_save_dict[backbone_type].items():\n",
    "        classifiermasked_save_dict_path=f'results/7_classifiermasked/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(classifiermasked_save_dict_path):\n",
    "            with open(classifiermasked_save_dict_path, 'rb') as f:\n",
    "                classifiermasked_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            classifiermasked_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(classifiermasked_save_dict_backbone_method)            \n",
    "        len_loaded=len(classifiermasked_save_dict_loaded)\n",
    "        classifiermasked_save_dict_backbone_method.update(classifiermasked_save_dict_loaded)\n",
    "        len_updated=len(classifiermasked_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsedtime_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    elapsedtime_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    elapsedtime_save_dict[backbone_type]=elapsedtime_save_dict_backbone\n",
    "    \n",
    "def elapsedtime_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, elapsed_time_list,\n",
    "                                 shape=None):\n",
    "    elapsedtime_save_dict_backbone_method=elapsedtime_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(elapsed_time_list)\n",
    "    \n",
    "    for elapsed_time, path in zip(elapsed_time_list, path_list):\n",
    "        assert type(elapsed_time)==float\n",
    "        assert type(path)==str\n",
    "        elapsedtime_save_dict_backbone_method[path]={\"time\": elapsed_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50926815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, elapsedtime_save_dict_backbone_method in elapsedtime_save_dict[backbone_type].items():\n",
    "        elapsedtime_save_dict_path=f'results/8_elapsedtime/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(elapsedtime_save_dict_path):\n",
    "            with open(elapsedtime_save_dict_path, 'rb') as f:\n",
    "                elapsedtime_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            elapsedtime_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(elapsedtime_save_dict_backbone_method)            \n",
    "        len_loaded=len(elapsedtime_save_dict_loaded)\n",
    "        elapsedtime_save_dict_backbone_method.update(elapsedtime_save_dict_loaded)\n",
    "        len_updated=len(elapsedtime_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf838f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    estimationerror_save_dict_backbone={\"kernelshap\":{},\n",
    "                                        \"kernelshapnopair\":{},\n",
    "                                        \"ours\": {}\n",
    "                                        }\n",
    "    estimationerror_save_dict[backbone_type]=estimationerror_save_dict_backbone\n",
    "    \n",
    "def estimationerror_save_dict_update(backbone_type, explanation_method,\n",
    "                                     path_list, estimation_list, label_list,\n",
    "                                     shape=None):\n",
    "    estimationerror_save_dict_backbone_method=estimationerror_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(estimation_list) == len(label_list)\n",
    "    \n",
    "    for path, estimation, label in zip(path_list, estimation_list, label_list):\n",
    "        assert type(path)==str\n",
    "        #assert type(estimation)==np.ndarray\n",
    "        assert type(label)==int\n",
    "        \n",
    "        if shape is not None:\n",
    "            assert estimation.shape==shape        \n",
    "        \n",
    "        estimationerror_save_dict_backbone_method[path]={\"estimation\": estimation,\n",
    "                                                         \"label\": label}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "        estimationerror_save_dict_path=f'results/9_estimationerror/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(estimationerror_save_dict_path):\n",
    "            with open(estimationerror_save_dict_path, 'rb') as f:\n",
    "                estimationerror_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            estimationerror_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(estimationerror_save_dict_backbone_method)            \n",
    "        len_loaded=len(estimationerror_save_dict_loaded)\n",
    "        estimationerror_save_dict_backbone_method.update(estimationerror_save_dict_loaded)\n",
    "        len_updated=len(estimationerror_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c1111",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16405739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_value(x, random_seed=None):\n",
    "    assert len(x.shape)==1\n",
    "    \n",
    "    if isinstance(random_seed, int):\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        perm = rng.permutation(np.arange(len(x)))\n",
    "    else:\n",
    "        perm = np.random.permutation(np.arange(len(x)))    \n",
    "\n",
    "    argsorted=np.arange(len(x))[perm][np.argsort(x[perm])]\n",
    "    relative_value=np.argsort(argsorted)\n",
    "\n",
    "    return relative_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "252a017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_path(path_original, dict_keys):\n",
    "    path_list = ['l0.cs.washington.edu', 'l1lambda.cs.washington.edu', 'l2lambda.cs.washington.edu',\n",
    "                 'l3.cs.washington.edu', 'deeper.cs.washington.edu', 'sync', '/homes/gws/chanwkim/', '/mmfs1/home/chanwkim/']\n",
    "    dict_keys=list(dict_keys)\n",
    "\n",
    "\n",
    "    for path1 in path_list:\n",
    "        if path1 in path_original:\n",
    "            for path2 in path_list:\n",
    "                path_replaced=path_original.replace(path1, path2)\n",
    "                if path_replaced in dict_keys:\n",
    "                    return path_replaced\n",
    "    return path_original\n",
    "    #raise ValueError(f\"not found {path_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1c66",
   "metadata": {},
   "source": [
    "# Methods to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b71c18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random', 'attention_rollout', 'attention_last', 'LRP', 'gradcam', 'gradcamgithub', 'vanillapixel', 'vanillaembedding', 'sgpixel', 'sgembedding', 'vargradpixel', 'vargradembedding', 'igpixel', 'igembedding', 'leaveoneoutclassifier', 'riseclassifier', 'ours']\n"
     ]
    }
   ],
   "source": [
    "explanation_method_to_run_=[\"random\", \"attention_rollout\", \"attention_last\", \n",
    "                            \"LRP\", \"gradcam\", \"gradcamgithub\",\n",
    "                            \"vanillapixel\", \"vanillaembedding\",\n",
    "                            \"sgpixel\", \"sgembedding\",\n",
    "                            \"vargradpixel\", \"vargradembedding\",\n",
    "                            \"igpixel\", \"igembedding\",                           \n",
    "                            \"leaveoneoutclassifier\",\n",
    "                            \"riseclassifier\", \n",
    "                            \"ours\"]\n",
    "#explanation_method_to_run_=[\"kernelshap\"]\n",
    "# explanation_method_to_run_=[\"random\", \"attention_rollout\", \"attention_last\", \n",
    "#                             \"LRP\", \"gradcam\", \n",
    "#                             \"vanillaembedding\",\n",
    "#                             \"sgembedding\",\n",
    "#                             \"vargradembedding\",\n",
    "#                             \"igembedding\",                           \n",
    "#                             \"leaveoneoutclassifier\",\n",
    "#                             \"riseclassifier\", \n",
    "#                             \"ours\"]\n",
    "explanation_method_to_run=[]\n",
    "explanation_method_to_run+=explanation_method_to_run_[:]\n",
    "\n",
    "\n",
    "print(explanation_method_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213d97c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "1963\n"
     ]
    }
   ],
   "source": [
    "data_loader=DataLoader(dset, batch_size=1, shuffle=False, drop_last=False, num_workers=4) #16\n",
    "print(len(dset))\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b09d1",
   "metadata": {},
   "source": [
    "# 1_classifier_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4328d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"1_classifier_evaluate\":    \n",
    "    classifier_result_list_all={}\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_result_list_all[backbone_type]={}\n",
    "           \n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader)):\n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)\n",
    "            if _config[\"output_dim\"]==1:\n",
    "                prob=classifier_output['logits'].sigmoid().cpu().numpy()\n",
    "            else:\n",
    "                prob=classifier_output['logits'].softmax(dim=-1).cpu().numpy()          \n",
    "                \n",
    "                \n",
    "            for path, label, prob in zip(paths, labels, prob):\n",
    "                classifier_result_list_all[backbone_type][path]={'label':label.item(), 'prob':prob.astype(float)}\n",
    "                \n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_result_list_path=f'results/1_classifier_evaluate/{_config[\"datasets\"]}/{backbone_type}_{dataset_split}.pickle'\n",
    "        with open(classifier_result_list_path, \"wb\") as f:\n",
    "            pickle.dump(classifier_result_list_all[backbone_type], f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d237daa",
   "metadata": {},
   "source": [
    "# 2_surrogate_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"2_surrogate_evaluate\":\n",
    "    result_list_all={}\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        result_list_all[backbone_type]=[]\n",
    "\n",
    "    dset_loader=DataLoader(dset, batch_size=64, num_workers=4, shuffle=False, drop_last=True)\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(dset_loader, unit='batch')):  \n",
    "        for num_mask in range(0,196+1,14):\n",
    "            mask=torch.zeros((len(batch[\"images\"]), 196))\n",
    "            mask[:,:num_mask]=1\n",
    "            for i in range(len(mask)):\n",
    "                mask[i]=mask[i][torch.randperm(len(mask[i]))]\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                surrogate_dict[backbone_type][\"original\"].eval()\n",
    "                with torch.no_grad():\n",
    "                    out_original=surrogate_dict[backbone_type][\"original\"](batch[\"images\"].to(surrogate_dict[backbone_type][\"original\"].device),\n",
    "                                                                          torch.ones((len(batch[\"images\"]), 196)).to(surrogate_dict[backbone_type][\"original\"].device))\n",
    "\n",
    "                for mask_location_model in [\"original\" , \"pre-softmax\", \"zero-input\", \"zero-embedding\"]:\n",
    "                    if mask_location_model==\"original\":\n",
    "                        kl_divergence=0\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            accuracy=((out_original[\"logits\"].sigmoid()>0.5).cpu().int()==batch['labels']).float().mean().item()\n",
    "                        else:\n",
    "                            accuracy=(torch.argmax(out_original[\"logits\"], dim=1).cpu()==batch['labels']).float().mean().item()\n",
    "\n",
    "                        result_list_all[backbone_type].append({\"batch_idx\": batch_idx,\n",
    "                                            \"backbone_type\": backbone_type,\n",
    "                                            \"num_mask\": num_mask,\n",
    "                                            \"mask_location_model\": mask_location_model,\n",
    "                                            \"mask_location_parameter\": \"original\",\n",
    "                                            \"kl_divergence\": kl_divergence,\n",
    "                                            \"accuracy\": accuracy})\n",
    "\n",
    "                    for mask_location_parameter in [\"pre-softmax\", \"post-softmax\", \"zero-input\", \"zero-embedding\", \"random-sampling\"]:\n",
    "                        surrogate_dict[backbone_type][mask_location_model].eval()\n",
    "                        with torch.no_grad():\n",
    "                            out_surrogate=surrogate_dict[backbone_type][mask_location_model](batch[\"images\"].to(surrogate_dict[backbone_type][mask_location_model].device), \n",
    "                                                                                             mask.to(surrogate_dict[backbone_type][mask_location_model].device),\n",
    "                                                                                             mask_location_parameter)\n",
    "\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            kl_divergence = F.kl_div(input=torch.concat([F.logsigmoid(out_surrogate[\"logits\"]), F.logsigmoid(-out_surrogate[\"logits\"])], dim=1),\n",
    "                                                    target=torch.concat([torch.sigmoid(out_original[\"logits\"]), torch.sigmoid(-out_original[\"logits\"])], dim=1),\n",
    "                                                    reduction=\"batchmean\",\n",
    "                                                    log_target=False)                        \n",
    "\n",
    "                        else:\n",
    "                            kl_divergence=F.kl_div(input=torch.log_softmax(out_surrogate[\"logits\"], dim=1),\n",
    "                                                   target=torch.softmax(out_original[\"logits\"], dim=1),\n",
    "                                                   log_target=False,\n",
    "                                                   reduction='batchmean').item()                           \n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            accuracy=((out_surrogate[\"logits\"].sigmoid()>0.5).cpu().int()==batch['labels']).float().mean().item()\n",
    "                        else:\n",
    "                            accuracy=(torch.argmax(out_surrogate[\"logits\"], dim=1).cpu()==batch['labels']).float().mean().item()\n",
    "\n",
    "\n",
    "                        result_list_all[backbone_type].append({\"batch_idx\": batch_idx,\n",
    "                                            \"backbone_type\": backbone_type,\n",
    "                                            \"num_mask\": num_mask,\n",
    "                                            \"mask_location_model\": mask_location_model,\n",
    "                                            \"mask_location_parameter\": mask_location_parameter,\n",
    "                                            \"kl_divergence\": kl_divergence,\n",
    "                                            \"accuracy\": accuracy})\n",
    "                        \n",
    "                        \n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        result_df=pd.DataFrame(result_list_all[backbone_type])\n",
    "\n",
    "        result_df.to_csv(f'results/4_0_surrogate_evaluate/{_config[\"datasets\"]}/{backbone_type}.csv')                            \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352d73b",
   "metadata": {},
   "source": [
    "# 3_explanation_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adapt_path(path_original, path_format):\n",
    "#     path_list=['l0.cs.washington.edu', 'l1lambda.cs.washington.edu', 'l2lambda.cs.washington.edu', 'l3.cs.washington.edu', 'deeper.cs.washington.edu']\n",
    "    \n",
    "#     for path1 in path_list:\n",
    "#         if path1 in path_original:\n",
    "#             for path2 in path_list:\n",
    "#                 if path2 in path_format:\n",
    "#                     return path_original.replace(path1, path2)\n",
    "#             raise\n",
    "#     return path_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe91774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_explanation(num_players, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_players,))\n",
    "    else:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_samples, num_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439314e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_use=['Garbage truck', \n",
    "              'Tench', \n",
    "              'English springer', \n",
    "              'Parachute',  \n",
    "              'Golf ball', \n",
    "              'Gas pump']\n",
    "kernelshap_sample_idx_list_all=[]\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for random_seed in [2, 3, 4, 5]:\n",
    "        label_data_list=np.array([i['label'] for i in dset.data])\n",
    "        kernelshap_sample_idx_list=[np.random.RandomState(random_seed).choice(np.arange(len(label_data_list))[(label_data_list==label_idx)]) for label_idx in [label_name_list.index(label) for label in label_to_use]]\n",
    "        kernelshap_sample_idx_list_all+=kernelshap_sample_idx_list\n",
    "kernelshap_sample_path_list_all=[dset[i]['path'] for i in kernelshap_sample_idx_list_all]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478944c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"3_explanation_generate\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "#         if dataset_split==\"test\":\n",
    "#             if idx>int(1000/data_loader.batch_size+0.5):\n",
    "#                 break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=explanation_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                if explanation_method==\"random\":\n",
    "                    start_time=time.time()\n",
    "                    explanation_random_list=[get_random_explanation(num_players=196) for path in paths]\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'random', path_list=paths, explanation_list=explanation_random_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196, ))\n",
    "                \n",
    "                elif explanation_method==\"attention_rollout\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_rollout_list=attentions_to_explanation(attentions, mode='rollout')\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'attention_rollout', path_list=paths, explanation_list=explanation_attention_rollout_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196,))\n",
    "\n",
    "                elif explanation_method==\"attention_last\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_last_list=attentions_to_explanation(attentions, mode=-1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'attention_last', path_list=paths, explanation_list=explanation_attention_last_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196,))            \n",
    "\n",
    "                elif explanation_method==\"LRP\":\n",
    "                    explanation_lrp_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_lrp_list.append(np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                              original_image=image.squeeze(0),\n",
    "                                                                              class_index=i,\n",
    "                                                                              mode='transformer_attribution').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0))\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'LRP', path_list=paths, explanation_list=explanation_lrp_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"gradcam\":\n",
    "                    explanation_gradcam_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcam = np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                                              original_image=image.squeeze(0),\n",
    "                                                                                              class_index=i,\n",
    "                                                                                              mode='attn_gradcam').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcam = np.nan_to_num(explanation_gradcam,nan=0)+np.random.uniform(low=0, high=1e-20, size=explanation_gradcam.shape)                    \n",
    "                        explanation_gradcam_list.append(explanation_gradcam)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'gradcam', path_list=paths, explanation_list=explanation_gradcam_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "\n",
    "                elif explanation_method==\"gradcamgithub\":\n",
    "                    explanation_gradcamgithub_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcamgithub = np.concatenate([cam_dict[backbone_type](input_tensor=image.unsqueeze(0).to(next(cam_dict[backbone_type].model.parameters()).device),\n",
    "                                                                                            targets=[ClassifierOutputTarget(i)], resize=False).flatten()[np.newaxis,:] for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcamgithub_list.append(explanation_gradcamgithub)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'gradcamgithub', path_list=paths, explanation_list=explanation_gradcamgithub_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vanillapixel\":\n",
    "                    explanation_vanillapixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_pixel=saliency_pixel_dict[backbone_type])\n",
    "                        explanation_vanillapixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vanillapixel', path_list=paths, explanation_list=explanation_vanillapixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vanillaembedding\":\n",
    "                    explanation_vanillaembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:\n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_embedding=saliency_embedding_dict[backbone_type])\n",
    "                        explanation_vanillaembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vanillaembedding', path_list=paths, explanation_list=explanation_vanillaembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"sgpixel\":\n",
    "                    explanation_sgpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_sgpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'sgpixel', path_list=paths, explanation_list=explanation_sgpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "\n",
    "                elif explanation_method==\"sgembedding\":\n",
    "                    explanation_sgembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_sgembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'sgembedding', path_list=paths, explanation_list=explanation_sgembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                                \n",
    "\n",
    "                elif explanation_method==\"vargradpixel\":\n",
    "                    explanation_vargradpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_vargradpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vargradpixel', path_list=paths, explanation_list=explanation_vargradpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vargradembedding\":\n",
    "                    explanation_vargradembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_vargradembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vargradembedding', path_list=paths, explanation_list=explanation_vargradembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                                \n",
    "\n",
    "                elif explanation_method==\"igpixel\":\n",
    "                    explanation_igpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_pixel=ig_pixel_dict[backbone_type])\n",
    "                        explanation_igpixel_list.append(grad[\"attributions_pixel_patchsum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'igpixel', path_list=paths, explanation_list=explanation_igpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"igembedding\":\n",
    "                    explanation_igembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_embedding=ig_embedding_dict[backbone_type])\n",
    "                        explanation_igembedding_list.append(grad[\"attributions_embedding_sum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'igembedding', path_list=paths, explanation_list=explanation_igembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "\n",
    "                elif explanation_method==\"leaveoneoutclassifier\":\n",
    "                    explanation_leaveoneoutclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutclassifier = leave_one_out(classifier=classifier_dict_[backbone_type], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutclassifier_list.append(explanation_leaveoneoutclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'leaveoneoutclassifier', path_list=paths, explanation_list=explanation_leaveoneoutclassifier_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"leaveoneoutsurrogate\":\n",
    "                    explanation_leaveoneoutsurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutsurrogate = leave_one_out(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutsurrogate_list.append(explanation_leaveoneoutsurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'leaveoneoutsurrogate', path_list=paths, explanation_list=explanation_leaveoneoutsurrogate_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "\n",
    "                elif explanation_method==\"riseclassifier\":\n",
    "                    explanation_riseclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_riseclassifier = rise(classifier=classifier_dict_[backbone_type], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_riseclassifier_list.append(explanation_riseclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'riseclassifier', path_list=paths, explanation_list=explanation_riseclassifier_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"risesurrogate\":\n",
    "                    explanation_risesurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_risesurrogate = rise(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_risesurrogate_list.append(explanation_risesurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'risesurrogate', path_list=paths, explanation_list=explanation_risesurrogate_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "                    \n",
    "                elif explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'ours', path_list=paths, explanation_list=explanation_ours, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(_config[\"output_dim\"], 196))                                \n",
    "                    \n",
    "                elif explanation_method==\"kernelshap\":                    \n",
    "                    explanation_kernelshap_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    path_list=[]\n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path,image in zip(paths, images):\n",
    "                        if path not in kernelshap_sample_path_list_all:\n",
    "                            continue\n",
    "                        print(path)\n",
    "                        start_time=time.time()                        \n",
    "                        explanation_kernelshap_ret = get_shap(surrogate_SHAP_wrapped_dict[backbone_type], image, thresh=0.2)\n",
    "                        explanation_kernelshap = explanation_kernelshap_ret.values.T\n",
    "                        explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                        path_list.append(path)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'kernelshap', path_list=path_list, explanation_list=explanation_kernelshap_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                    \n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "                    explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(explanation_save_dict_path):\n",
    "                        try:\n",
    "                            with open(explanation_save_dict_path, 'rb') as f:\n",
    "                                explanation_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            explanation_save_dict_loaded={}\n",
    "                    else:\n",
    "                        explanation_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(explanation_save_dict_backbone_method)            \n",
    "                    len_loaded=len(explanation_save_dict_loaded)\n",
    "                    explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "                    len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "                    with open(explanation_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(explanation_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"3_explanation_generate\":\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "            explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(explanation_save_dict_path):\n",
    "                with open(explanation_save_dict_path, 'rb') as f:\n",
    "                    explanation_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                explanation_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(explanation_save_dict_backbone_method)            \n",
    "            len_loaded=len(explanation_save_dict_loaded)\n",
    "            explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "            len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "            with open(explanation_save_dict_path, \"wb\") as f:\n",
    "                pickle.dump(explanation_save_dict_backbone_method, f)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcab1c5",
   "metadata": {},
   "source": [
    "# 4_insert_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1c78578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_to_mask(explanation, mode='insertion'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        explanation: (num_batches, num_players)\n",
    "    Returns:\n",
    "        explanation_expaned_bool: (num_batches, num_players+1, num_players)\n",
    "    \"\"\"\n",
    "    \n",
    "    explanation_expaned=np.repeat(explanation[:,np.newaxis,:], explanation.shape[-1], axis=1) # (num_batches, num_players, num_players)\n",
    "    \n",
    "    if mode=='insertion':\n",
    "        explanation_expaned_bool = explanation_expaned > ((np.sort(explanation, axis=-1)[:, : :-1])[:, :, np.newaxis]) # (num_batches, num_players, num_players)\n",
    "        explanation_expaned_bool = np.concatenate([explanation_expaned_bool,\n",
    "                                                   np.ones(shape=(explanation_expaned_bool.shape[0], 1, explanation_expaned_bool.shape[2]))==1], axis=1) # (num_batches, num_players+1, num_players)\n",
    "        #print(explanation_expaned_bool.shape)\n",
    "    elif mode=='deletion':\n",
    "        explanation_expaned_bool = explanation_expaned < ((np.sort(explanation, axis=-1)[:, : :-1])[:, :, np.newaxis]) # (num_batches, num_players, num_players)\n",
    "        explanation_expaned_bool = np.concatenate([np.ones(shape=(explanation_expaned_bool.shape[0], 1, explanation_expaned_bool.shape[2]))==1,\n",
    "                                                   explanation_expaned_bool],axis=1) # (num_batches, num_players+1, num_players)        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f'{mode} should be insertion or deletion.')\n",
    "    \n",
    "    \n",
    "    return explanation_expaned_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "835e5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_stage=\"4_insert_delete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_method_to_run=[\"kernelshap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e30e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_sample_path_list=pd.DataFrame(data_loader.dataset.data).groupby(\"label\").apply(lambda x: x.sample(n=10, random_state=42))[\"img_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cb8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_mode=(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[path in data_keys for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c028ed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4351.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_19792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4062.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/ILSVRC2012_val_00017020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10200.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_3260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8971.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18040.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_930.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_13871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3651.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_37161.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_24352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/ILSVRC2012_val_00022252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11481.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16220.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9940.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_23321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4220.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_121.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7982.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/ILSVRC2012_val_00026451.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8522.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48060.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7790.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_29712.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_3241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30001.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18981.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7841.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12972.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9642.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11210.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4900.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20360.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11091.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_49041.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_521.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20301.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14910.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_28352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4511.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10451.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_2511.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_5820.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_31592.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_931.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_3281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00025761.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12291.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15571.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_3532.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_15262.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00024560.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_2390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2122.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_672.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5712.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_15441.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_24332.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_17460.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_24552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1222.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_32350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_5781.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1132.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13600.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00030740.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16861.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39880.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_41101.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5641.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8081.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4131.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_27102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11081.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_8330.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6622.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15810.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_28600.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15701.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35172.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6421.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4980.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_29062.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_11701.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3932.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_23510.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_71550.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11401.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_29580.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_261.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1002.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15152.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10230.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10592.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18622.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9821.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_72470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8240.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6882.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_4691.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_37571.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7372.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_562.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48881.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_26102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6180.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_51440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_9770.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12831.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19842.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11000.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13480.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_911.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_20281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_20061.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3940.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4310.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11452.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6752.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16920.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16051.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_29231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_19501.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_29462.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1100.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38841.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_7292.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_230.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_11092.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_5362.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10271.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12632.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6811.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4492.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14351.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38201.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38680.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10491.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2730.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3471.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_4320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2822.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_41871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10692.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7772.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_17192.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19472.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_17782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_19542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3922.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_5231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_27662.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2342.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/ILSVRC2012_val_00038942.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_2270.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_46700.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_30141.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5852.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00004301.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_13541.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8610.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5690.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_73490.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14112.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_192.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_23440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12802.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/ILSVRC2012_val_00035211.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4411.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3530.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13250.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15291.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8891.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4731.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_52232.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_1792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_24941.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2490.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00027110.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_532.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_28350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_22681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_22390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_24391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_28400.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_5641.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4852.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_42671.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3061.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2572.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_24681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_3780.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2402.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/ILSVRC2012_val_00008162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18152.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1630.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17690.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_11470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_56022.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/ILSVRC2012_val_00023440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_43251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14682.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5731.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4972.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_31961.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5370.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_38560.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_9300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16280.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/ILSVRC2012_val_00035160.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_362.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_16370.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1660.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_482.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2340.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_501.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1621.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20500.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18590.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12170.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6160.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_11190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_20572.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5280.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_2170.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6662.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30671.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_31710.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7092.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14870.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8661.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17851.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5120.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26541.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00022172.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_67480.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_880.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10141.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_20382.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_6251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_35890.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_5090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3890.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20742.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_15312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_17001.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3030.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1822.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15820.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2382.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_23312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_20052.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13831.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18430.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3171.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_62551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13770.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_13542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7860.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30881.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_14531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_50380.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_10210.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_63471.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_8320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_41342.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19060.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10151.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_42422.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_21161.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8611.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00009651.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_3142.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_19390.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8911.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21730.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18450.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_26260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6490.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_10121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00021740.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_20620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_73320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9811.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_16080.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_1980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8801.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4262.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13582.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3242.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1960.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_23571.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_15731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3722.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26631.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14181.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38212.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_24542.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6881.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21032.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/ILSVRC2012_val_00017801.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_7160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_2841.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_23971.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19570.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_16912.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_28830.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_560.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3062.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_14362.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_17272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_18592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_22302.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1350.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_13702.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14992.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_19661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15130.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_6971.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32422.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_22661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_4341.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7822.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13672.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1230.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_34492.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_49281.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_8112.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1292.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4752.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_901.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_791.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7002.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5871.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5501.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17872.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_23272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15162.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10230.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_562.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4382.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17031.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_61581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1821.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12122.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_931.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_33182.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7931.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9662.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6710.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1530.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1262.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/ILSVRC2012_val_00047060.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_6031.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_36541.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_1100.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2170.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_72982.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_34632.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_29910.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_33221.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_27231.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6081.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7022.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6042.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_60232.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17862.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_3932.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1962.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_36380.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_72272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_27252.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18042.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5680.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_621.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35271.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11642.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_34280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1770.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1881.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1842.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_29410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19261.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1300.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_65922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_19272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/ILSVRC2012_val_00020502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_1951.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10762.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1332.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1850.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_58270.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5311.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_44580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_9301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_24502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14860.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7310.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9440.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_27010.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12732.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6882.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2481.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8172.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2930.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1450.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3492.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1271.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_23421.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_76121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_16081.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4622.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_26802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_910.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_11241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/ILSVRC2012_val_00043731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_22761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11602.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_76721.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2941.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12370.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5402.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_40411.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_821.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5222.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_34132.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16822.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20651.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_31181.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1561.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_15511.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5861.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14900.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_20312.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3972.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9431.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_20572.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5851.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6552.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1172.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1372.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8052.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_13681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6201.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_26852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12140.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12330.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_59361.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_5852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6770.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_33021.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_10612.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10782.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_47472.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13672.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3601.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_16952.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5890.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10760.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_6912.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1122.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11120.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_652.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_17521.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_43260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38050.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3200.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_31790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_14002.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8420.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_13601.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_5551.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7960.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12051.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1781.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_27320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14600.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_422.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3311.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2221.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1791.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8831.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8630.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10342.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1840.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_70.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_17060.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_108321.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3321.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2920.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2162.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_20232.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_7951.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_32580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_151.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_142.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16392.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11561.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_28352.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12430.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1631.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6392.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_12861.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6520.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3371.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_5732.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4342.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9981.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8901.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1022.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7730.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_26270.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11372.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19282.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_630.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7360.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13442.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9481.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48491.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_251.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1000.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_531.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26892.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9740.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_11331.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8641.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9371.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_760.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_350.JPEG']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d5ceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4351.JPEG']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ff78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "/homes/gws/chanwkim/, /mmfs1/home/chanwkim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b8727f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|                                      | 691/1000 [00:06<00:02, 110.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|                                      | 691/1000 [00:18<00:02, 110.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       678   +    680   ->    681\n",
      "attention_rollout            678   +    679   ->    680\n",
      "attention_last               678   +    679   ->    680\n",
      "LRP                          678   +    679   ->    680\n",
      "gradcam                      678   +    679   ->    680\n",
      "gradcamgithub                678   +    679   ->    680\n",
      "vanillapixel                 678   +    679   ->    680\n",
      "vanillaembedding             678   +    679   ->    680\n",
      "sgpixel                      678   +    679   ->    680\n",
      "sgembedding                  678   +    679   ->    680\n",
      "vargradpixel                 678   +    679   ->    680\n",
      "vargradembedding             678   +    679   ->    680\n",
      "igpixel                      678   +    679   ->    680\n",
      "igembedding                  678   +    679   ->    680\n",
      "leaveoneoutclassifier        678   +    679   ->    680\n",
      "riseclassifier               678   +    679   ->    680\n",
      "ours                         678   +    679   ->    680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|                                     | 695/1000 [14:27<2:16:52, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       682   +    685   ->    686\n",
      "attention_rollout            681   +    684   ->    685\n",
      "attention_last               681   +    684   ->    685\n",
      "LRP                          681   +    684   ->    685\n",
      "gradcam                      681   +    684   ->    685\n",
      "gradcamgithub                681   +    684   ->    685\n",
      "vanillapixel                 681   +    684   ->    685\n",
      "vanillaembedding             681   +    684   ->    685\n",
      "sgpixel                      681   +    684   ->    685\n",
      "sgembedding                  681   +    684   ->    685\n",
      "vargradpixel                 681   +    684   ->    685\n",
      "vargradembedding             681   +    684   ->    685\n",
      "igpixel                      681   +    684   ->    685\n",
      "igembedding                  681   +    684   ->    685\n",
      "leaveoneoutclassifier        681   +    684   ->    685\n",
      "riseclassifier               681   +    684   ->    685\n",
      "ours                         681   +    684   ->    685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|                                     | 699/1000 [28:53<4:39:10, 55.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       687   +    689   ->    690\n",
      "attention_rollout            686   +    688   ->    689\n",
      "attention_last               686   +    688   ->    689\n",
      "LRP                          686   +    688   ->    689\n",
      "gradcam                      686   +    688   ->    689\n",
      "gradcamgithub                686   +    688   ->    689\n",
      "vanillapixel                 686   +    688   ->    689\n",
      "vanillaembedding             686   +    688   ->    689\n",
      "sgpixel                      686   +    688   ->    689\n",
      "sgembedding                  686   +    688   ->    689\n",
      "vargradpixel                 686   +    688   ->    689\n",
      "vargradembedding             686   +    688   ->    689\n",
      "igpixel                      686   +    688   ->    689\n",
      "igembedding                  686   +    688   ->    689\n",
      "leaveoneoutclassifier        686   +    688   ->    689\n",
      "riseclassifier               686   +    688   ->    689\n",
      "ours                         686   +    688   ->    689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|                                    | 703/1000 [43:18<6:56:56, 84.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       691   +    693   ->    694\n",
      "attention_rollout            690   +    692   ->    693\n",
      "attention_last               690   +    692   ->    693\n",
      "LRP                          690   +    692   ->    693\n",
      "gradcam                      690   +    692   ->    693\n",
      "gradcamgithub                690   +    692   ->    693\n",
      "vanillapixel                 690   +    692   ->    693\n",
      "vanillaembedding             690   +    692   ->    693\n",
      "sgpixel                      690   +    692   ->    693\n",
      "sgembedding                  690   +    692   ->    693\n",
      "vargradpixel                 690   +    692   ->    693\n",
      "vargradembedding             690   +    692   ->    693\n",
      "igpixel                      690   +    692   ->    693\n",
      "igembedding                  690   +    692   ->    693\n",
      "leaveoneoutclassifier        690   +    692   ->    693\n",
      "riseclassifier               690   +    692   ->    693\n",
      "ours                         690   +    692   ->    693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|                                    | 707/1000 [57:41<9:01:18, 110.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       695   +    697   ->    698\n",
      "attention_rollout            694   +    696   ->    697\n",
      "attention_last               694   +    696   ->    697\n",
      "LRP                          694   +    696   ->    697\n",
      "gradcam                      694   +    696   ->    697\n",
      "gradcamgithub                694   +    696   ->    697\n",
      "vanillapixel                 694   +    696   ->    697\n",
      "vanillaembedding             694   +    696   ->    697\n",
      "sgpixel                      694   +    696   ->    697\n",
      "sgembedding                  694   +    696   ->    697\n",
      "vargradpixel                 694   +    696   ->    697\n",
      "vargradembedding             694   +    696   ->    697\n",
      "igpixel                      694   +    696   ->    697\n",
      "igembedding                  694   +    696   ->    697\n",
      "leaveoneoutclassifier        694   +    696   ->    697\n",
      "riseclassifier               694   +    696   ->    697\n",
      "ours                         694   +    696   ->    697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|                                  | 711/1000 [1:12:06<10:47:58, 134.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       699   +    701   ->    702\n",
      "attention_rollout            698   +    700   ->    701\n",
      "attention_last               698   +    700   ->    701\n",
      "LRP                          698   +    700   ->    701\n",
      "gradcam                      698   +    700   ->    701\n",
      "gradcamgithub                698   +    700   ->    701\n",
      "vanillapixel                 698   +    700   ->    701\n",
      "vanillaembedding             698   +    700   ->    701\n",
      "sgpixel                      698   +    700   ->    701\n",
      "sgembedding                  698   +    700   ->    701\n",
      "vargradpixel                 698   +    700   ->    701\n",
      "vargradembedding             698   +    700   ->    701\n",
      "igpixel                      698   +    700   ->    701\n",
      "igembedding                  698   +    700   ->    701\n",
      "leaveoneoutclassifier        698   +    700   ->    701\n",
      "riseclassifier               698   +    700   ->    701\n",
      "ours                         698   +    700   ->    701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|                                  | 715/1000 [1:26:29<12:12:49, 154.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       703   +    705   ->    706\n",
      "attention_rollout            702   +    704   ->    705\n",
      "attention_last               702   +    704   ->    705\n",
      "LRP                          702   +    704   ->    705\n",
      "gradcam                      702   +    704   ->    705\n",
      "gradcamgithub                702   +    704   ->    705\n",
      "vanillapixel                 702   +    704   ->    705\n",
      "vanillaembedding             702   +    704   ->    705\n",
      "sgpixel                      702   +    704   ->    705\n",
      "sgembedding                  702   +    704   ->    705\n",
      "vargradpixel                 702   +    704   ->    705\n",
      "vargradembedding             702   +    704   ->    705\n",
      "igpixel                      702   +    704   ->    705\n",
      "igembedding                  702   +    704   ->    705\n",
      "leaveoneoutclassifier        702   +    704   ->    705\n",
      "riseclassifier               702   +    704   ->    705\n",
      "ours                         702   +    704   ->    705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|                                 | 719/1000 [1:40:55<13:17:26, 170.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       707   +    709   ->    710\n",
      "attention_rollout            706   +    708   ->    709\n",
      "attention_last               706   +    708   ->    709\n",
      "LRP                          706   +    708   ->    709\n",
      "gradcam                      706   +    708   ->    709\n",
      "gradcamgithub                706   +    708   ->    709\n",
      "vanillapixel                 706   +    708   ->    709\n",
      "vanillaembedding             706   +    708   ->    709\n",
      "sgpixel                      706   +    708   ->    709\n",
      "sgembedding                  706   +    708   ->    709\n",
      "vargradpixel                 706   +    708   ->    709\n",
      "vargradembedding             706   +    708   ->    709\n",
      "igpixel                      706   +    708   ->    709\n",
      "igembedding                  706   +    708   ->    709\n",
      "leaveoneoutclassifier        706   +    708   ->    709\n",
      "riseclassifier               706   +    708   ->    709\n",
      "ours                         706   +    708   ->    709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|                                 | 723/1000 [1:55:18<14:02:31, 182.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       711   +    713   ->    714\n",
      "attention_rollout            710   +    712   ->    713\n",
      "attention_last               710   +    712   ->    713\n",
      "LRP                          710   +    712   ->    713\n",
      "gradcam                      710   +    712   ->    713\n",
      "gradcamgithub                710   +    712   ->    713\n",
      "vanillapixel                 710   +    712   ->    713\n",
      "vanillaembedding             710   +    712   ->    713\n",
      "sgpixel                      710   +    712   ->    713\n",
      "sgembedding                  710   +    712   ->    713\n",
      "vargradpixel                 710   +    712   ->    713\n",
      "vargradembedding             710   +    712   ->    713\n",
      "igpixel                      710   +    712   ->    713\n",
      "igembedding                  710   +    712   ->    713\n",
      "leaveoneoutclassifier        710   +    712   ->    713\n",
      "riseclassifier               710   +    712   ->    713\n",
      "ours                         710   +    712   ->    713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|                                | 727/1000 [2:09:40<14:32:14, 191.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       715   +    717   ->    718\n",
      "attention_rollout            714   +    716   ->    717\n",
      "attention_last               714   +    716   ->    717\n",
      "LRP                          714   +    716   ->    717\n",
      "gradcam                      714   +    716   ->    717\n",
      "gradcamgithub                714   +    716   ->    717\n",
      "vanillapixel                 714   +    716   ->    717\n",
      "vanillaembedding             714   +    716   ->    717\n",
      "sgpixel                      714   +    716   ->    717\n",
      "sgembedding                  714   +    716   ->    717\n",
      "vargradpixel                 714   +    716   ->    717\n",
      "vargradembedding             714   +    716   ->    717\n",
      "igpixel                      714   +    716   ->    717\n",
      "igembedding                  714   +    716   ->    717\n",
      "leaveoneoutclassifier        714   +    716   ->    717\n",
      "riseclassifier               714   +    716   ->    717\n",
      "ours                         714   +    716   ->    717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|                                | 731/1000 [2:24:02<14:49:36, 198.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       719   +    721   ->    722\n",
      "attention_rollout            718   +    720   ->    721\n",
      "attention_last               718   +    720   ->    721\n",
      "LRP                          718   +    720   ->    721\n",
      "gradcam                      718   +    720   ->    721\n",
      "gradcamgithub                718   +    720   ->    721\n",
      "vanillapixel                 718   +    720   ->    721\n",
      "vanillaembedding             718   +    720   ->    721\n",
      "sgpixel                      718   +    720   ->    721\n",
      "sgembedding                  718   +    720   ->    721\n",
      "vargradpixel                 718   +    720   ->    721\n",
      "vargradembedding             718   +    720   ->    721\n",
      "igpixel                      718   +    720   ->    721\n",
      "igembedding                  718   +    720   ->    721\n",
      "leaveoneoutclassifier        718   +    720   ->    721\n",
      "riseclassifier               718   +    720   ->    721\n",
      "ours                         718   +    720   ->    721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|                               | 735/1000 [2:38:25<14:58:31, 203.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       723   +    725   ->    726\n",
      "attention_rollout            722   +    724   ->    725\n",
      "attention_last               722   +    724   ->    725\n",
      "LRP                          722   +    724   ->    725\n",
      "gradcam                      722   +    724   ->    725\n",
      "gradcamgithub                722   +    724   ->    725\n",
      "vanillapixel                 722   +    724   ->    725\n",
      "vanillaembedding             722   +    724   ->    725\n",
      "sgpixel                      722   +    724   ->    725\n",
      "sgembedding                  722   +    724   ->    725\n",
      "vargradpixel                 722   +    724   ->    725\n",
      "vargradembedding             722   +    724   ->    725\n",
      "igpixel                      722   +    724   ->    725\n",
      "igembedding                  722   +    724   ->    725\n",
      "leaveoneoutclassifier        722   +    724   ->    725\n",
      "riseclassifier               722   +    724   ->    725\n",
      "ours                         722   +    724   ->    725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|                               | 739/1000 [2:52:48<15:00:27, 207.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       727   +    730   ->    731\n",
      "attention_rollout            726   +    729   ->    730\n",
      "attention_last               726   +    729   ->    730\n",
      "LRP                          726   +    729   ->    730\n",
      "gradcam                      726   +    729   ->    730\n",
      "gradcamgithub                726   +    729   ->    730\n",
      "vanillapixel                 726   +    729   ->    730\n",
      "vanillaembedding             726   +    729   ->    730\n",
      "sgpixel                      726   +    729   ->    730\n",
      "sgembedding                  726   +    729   ->    730\n",
      "vargradpixel                 726   +    729   ->    730\n",
      "vargradembedding             726   +    729   ->    730\n",
      "igpixel                      726   +    729   ->    730\n",
      "igembedding                  726   +    729   ->    730\n",
      "leaveoneoutclassifier        726   +    729   ->    730\n",
      "riseclassifier               726   +    729   ->    730\n",
      "ours                         726   +    729   ->    730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|                              | 743/1000 [3:07:14<14:58:50, 209.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       732   +    734   ->    735\n",
      "attention_rollout            731   +    733   ->    734\n",
      "attention_last               731   +    733   ->    734\n",
      "LRP                          731   +    733   ->    734\n",
      "gradcam                      731   +    733   ->    734\n",
      "gradcamgithub                731   +    733   ->    734\n",
      "vanillapixel                 731   +    733   ->    734\n",
      "vanillaembedding             731   +    733   ->    734\n",
      "sgpixel                      731   +    733   ->    734\n",
      "sgembedding                  731   +    733   ->    734\n",
      "vargradpixel                 731   +    733   ->    734\n",
      "vargradembedding             731   +    733   ->    734\n",
      "igpixel                      731   +    733   ->    734\n",
      "igembedding                  731   +    733   ->    734\n",
      "leaveoneoutclassifier        731   +    733   ->    734\n",
      "riseclassifier               731   +    733   ->    734\n",
      "ours                         731   +    733   ->    734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|                              | 747/1000 [3:21:38<14:52:32, 211.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       736   +    738   ->    739\n",
      "attention_rollout            735   +    737   ->    738\n",
      "attention_last               735   +    737   ->    738\n",
      "LRP                          735   +    737   ->    738\n",
      "gradcam                      735   +    737   ->    738\n",
      "gradcamgithub                735   +    737   ->    738\n",
      "vanillapixel                 735   +    737   ->    738\n",
      "vanillaembedding             735   +    737   ->    738\n",
      "sgpixel                      735   +    737   ->    738\n",
      "sgembedding                  735   +    737   ->    738\n",
      "vargradpixel                 735   +    737   ->    738\n",
      "vargradembedding             735   +    737   ->    738\n",
      "igpixel                      735   +    737   ->    738\n",
      "igembedding                  735   +    737   ->    738\n",
      "leaveoneoutclassifier        735   +    737   ->    738\n",
      "riseclassifier               735   +    737   ->    738\n",
      "ours                         735   +    737   ->    738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|                              | 751/1000 [3:36:04<14:44:10, 213.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       740   +    742   ->    743\n",
      "attention_rollout            739   +    741   ->    742\n",
      "attention_last               739   +    741   ->    742\n",
      "LRP                          739   +    741   ->    742\n",
      "gradcam                      739   +    741   ->    742\n",
      "gradcamgithub                739   +    741   ->    742\n",
      "vanillapixel                 739   +    741   ->    742\n",
      "vanillaembedding             739   +    741   ->    742\n",
      "sgpixel                      739   +    741   ->    742\n",
      "sgembedding                  739   +    741   ->    742\n",
      "vargradpixel                 739   +    741   ->    742\n",
      "vargradembedding             739   +    741   ->    742\n",
      "igpixel                      739   +    741   ->    742\n",
      "igembedding                  739   +    741   ->    742\n",
      "leaveoneoutclassifier        739   +    741   ->    742\n",
      "riseclassifier               739   +    741   ->    742\n",
      "ours                         739   +    741   ->    742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|                             | 755/1000 [3:50:27<14:33:25, 213.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       744   +    746   ->    747\n",
      "attention_rollout            743   +    745   ->    746\n",
      "attention_last               743   +    745   ->    746\n",
      "LRP                          743   +    745   ->    746\n",
      "gradcam                      743   +    745   ->    746\n",
      "gradcamgithub                743   +    745   ->    746\n",
      "vanillapixel                 743   +    745   ->    746\n",
      "vanillaembedding             743   +    745   ->    746\n",
      "sgpixel                      743   +    745   ->    746\n",
      "sgembedding                  743   +    745   ->    746\n",
      "vargradpixel                 743   +    745   ->    746\n",
      "vargradembedding             743   +    745   ->    746\n",
      "igpixel                      743   +    745   ->    746\n",
      "igembedding                  743   +    745   ->    746\n",
      "leaveoneoutclassifier        743   +    745   ->    746\n",
      "riseclassifier               743   +    745   ->    746\n",
      "ours                         743   +    745   ->    746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|                             | 759/1000 [4:04:50<14:21:14, 214.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       748   +    750   ->    751\n",
      "attention_rollout            747   +    749   ->    750\n",
      "attention_last               747   +    749   ->    750\n",
      "LRP                          747   +    749   ->    750\n",
      "gradcam                      747   +    749   ->    750\n",
      "gradcamgithub                747   +    749   ->    750\n",
      "vanillapixel                 747   +    749   ->    750\n",
      "vanillaembedding             747   +    749   ->    750\n",
      "sgpixel                      747   +    749   ->    750\n",
      "sgembedding                  747   +    749   ->    750\n",
      "vargradpixel                 747   +    749   ->    750\n",
      "vargradembedding             747   +    749   ->    750\n",
      "igpixel                      747   +    749   ->    750\n",
      "igembedding                  747   +    749   ->    750\n",
      "leaveoneoutclassifier        747   +    749   ->    750\n",
      "riseclassifier               747   +    749   ->    750\n",
      "ours                         747   +    749   ->    750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|                            | 763/1000 [4:19:12<14:08:17, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       752   +    754   ->    755\n",
      "attention_rollout            751   +    753   ->    754\n",
      "attention_last               751   +    753   ->    754\n",
      "LRP                          751   +    753   ->    754\n",
      "gradcam                      751   +    753   ->    754\n",
      "gradcamgithub                751   +    753   ->    754\n",
      "vanillapixel                 751   +    753   ->    754\n",
      "vanillaembedding             751   +    753   ->    754\n",
      "sgpixel                      751   +    753   ->    754\n",
      "sgembedding                  751   +    753   ->    754\n",
      "vargradpixel                 751   +    753   ->    754\n",
      "vargradembedding             751   +    753   ->    754\n",
      "igpixel                      751   +    753   ->    754\n",
      "igembedding                  751   +    753   ->    754\n",
      "leaveoneoutclassifier        751   +    753   ->    754\n",
      "riseclassifier               751   +    753   ->    754\n",
      "ours                         751   +    753   ->    754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|                            | 767/1000 [4:33:34<13:54:58, 215.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       756   +    758   ->    759\n",
      "attention_rollout            755   +    757   ->    758\n",
      "attention_last               755   +    757   ->    758\n",
      "LRP                          755   +    757   ->    758\n",
      "gradcam                      755   +    757   ->    758\n",
      "gradcamgithub                755   +    757   ->    758\n",
      "vanillapixel                 755   +    757   ->    758\n",
      "vanillaembedding             755   +    757   ->    758\n",
      "sgpixel                      755   +    757   ->    758\n",
      "sgembedding                  755   +    757   ->    758\n",
      "vargradpixel                 755   +    757   ->    758\n",
      "vargradembedding             755   +    757   ->    758\n",
      "igpixel                      755   +    757   ->    758\n",
      "igembedding                  755   +    757   ->    758\n",
      "leaveoneoutclassifier        755   +    757   ->    758\n",
      "riseclassifier               755   +    757   ->    758\n",
      "ours                         755   +    757   ->    758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|                           | 771/1000 [4:47:57<13:41:21, 215.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       760   +    762   ->    763\n",
      "attention_rollout            759   +    761   ->    762\n",
      "attention_last               759   +    761   ->    762\n",
      "LRP                          759   +    761   ->    762\n",
      "gradcam                      759   +    761   ->    762\n",
      "gradcamgithub                759   +    761   ->    762\n",
      "vanillapixel                 759   +    761   ->    762\n",
      "vanillaembedding             759   +    761   ->    762\n",
      "sgpixel                      759   +    761   ->    762\n",
      "sgembedding                  759   +    761   ->    762\n",
      "vargradpixel                 759   +    761   ->    762\n",
      "vargradembedding             759   +    761   ->    762\n",
      "igpixel                      759   +    761   ->    762\n",
      "igembedding                  759   +    761   ->    762\n",
      "leaveoneoutclassifier        759   +    761   ->    762\n",
      "riseclassifier               759   +    761   ->    762\n",
      "ours                         759   +    761   ->    762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|                           | 775/1000 [5:02:22<13:28:05, 215.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       768   +    770   ->    771\n",
      "attention_rollout            767   +    769   ->    770\n",
      "attention_last               767   +    769   ->    770\n",
      "LRP                          767   +    769   ->    770\n",
      "gradcam                      767   +    769   ->    770\n",
      "gradcamgithub                767   +    769   ->    770\n",
      "vanillapixel                 767   +    769   ->    770\n",
      "vanillaembedding             767   +    769   ->    770\n",
      "sgpixel                      767   +    769   ->    770\n",
      "sgembedding                  767   +    769   ->    770\n",
      "vargradpixel                 767   +    769   ->    770\n",
      "vargradembedding             767   +    769   ->    770\n",
      "igpixel                      767   +    769   ->    770\n",
      "igembedding                  767   +    769   ->    770\n",
      "leaveoneoutclassifier        767   +    769   ->    770\n",
      "riseclassifier               767   +    769   ->    770\n",
      "ours                         767   +    769   ->    770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|                          | 783/1000 [5:31:06<12:59:27, 215.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       772   +    774   ->    775\n",
      "attention_rollout            771   +    773   ->    774\n",
      "attention_last               771   +    773   ->    774\n",
      "LRP                          771   +    773   ->    774\n",
      "gradcam                      771   +    773   ->    774\n",
      "gradcamgithub                771   +    773   ->    774\n",
      "vanillapixel                 771   +    773   ->    774\n",
      "vanillaembedding             771   +    773   ->    774\n",
      "sgpixel                      771   +    773   ->    774\n",
      "sgembedding                  771   +    773   ->    774\n",
      "vargradpixel                 771   +    773   ->    774\n",
      "vargradembedding             771   +    773   ->    774\n",
      "igpixel                      771   +    773   ->    774\n",
      "igembedding                  771   +    773   ->    774\n",
      "leaveoneoutclassifier        771   +    773   ->    774\n",
      "riseclassifier               771   +    773   ->    774\n",
      "ours                         771   +    773   ->    774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|                         | 787/1000 [5:45:29<12:45:16, 215.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       776   +    778   ->    779\n",
      "attention_rollout            775   +    777   ->    778\n",
      "attention_last               775   +    777   ->    778\n",
      "LRP                          775   +    777   ->    778\n",
      "gradcam                      775   +    777   ->    778\n",
      "gradcamgithub                775   +    777   ->    778\n",
      "vanillapixel                 775   +    777   ->    778\n",
      "vanillaembedding             775   +    777   ->    778\n",
      "sgpixel                      775   +    777   ->    778\n",
      "sgembedding                  775   +    777   ->    778\n",
      "vargradpixel                 775   +    777   ->    778\n",
      "vargradembedding             775   +    777   ->    778\n",
      "igpixel                      775   +    777   ->    778\n",
      "igembedding                  775   +    777   ->    778\n",
      "leaveoneoutclassifier        775   +    777   ->    778\n",
      "riseclassifier               775   +    777   ->    778\n",
      "ours                         775   +    777   ->    778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|                         | 791/1000 [5:59:52<12:31:09, 215.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       780   +    782   ->    783\n",
      "attention_rollout            779   +    781   ->    782\n",
      "attention_last               779   +    781   ->    782\n",
      "LRP                          779   +    781   ->    782\n",
      "gradcam                      779   +    781   ->    782\n",
      "gradcamgithub                779   +    781   ->    782\n",
      "vanillapixel                 779   +    781   ->    782\n",
      "vanillaembedding             779   +    781   ->    782\n",
      "sgpixel                      779   +    781   ->    782\n",
      "sgembedding                  779   +    781   ->    782\n",
      "vargradpixel                 779   +    781   ->    782\n",
      "vargradembedding             779   +    781   ->    782\n",
      "igpixel                      779   +    781   ->    782\n",
      "igembedding                  779   +    781   ->    782\n",
      "leaveoneoutclassifier        779   +    781   ->    782\n",
      "riseclassifier               779   +    781   ->    782\n",
      "ours                         779   +    781   ->    782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|                        | 795/1000 [6:14:17<12:17:13, 215.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       784   +    786   ->    787\n",
      "attention_rollout            783   +    785   ->    786\n",
      "attention_last               783   +    785   ->    786\n",
      "LRP                          783   +    785   ->    786\n",
      "gradcam                      783   +    785   ->    786\n",
      "gradcamgithub                783   +    785   ->    786\n",
      "vanillapixel                 783   +    785   ->    786\n",
      "vanillaembedding             783   +    785   ->    786\n",
      "sgpixel                      783   +    785   ->    786\n",
      "sgembedding                  783   +    785   ->    786\n",
      "vargradpixel                 783   +    785   ->    786\n",
      "vargradembedding             783   +    785   ->    786\n",
      "igpixel                      783   +    785   ->    786\n",
      "igembedding                  783   +    785   ->    786\n",
      "leaveoneoutclassifier        783   +    785   ->    786\n",
      "riseclassifier               783   +    785   ->    786\n",
      "ours                         783   +    785   ->    786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|                        | 799/1000 [6:28:38<12:02:29, 215.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       788   +    790   ->    791\n",
      "attention_rollout            787   +    789   ->    790\n",
      "attention_last               787   +    789   ->    790\n",
      "LRP                          787   +    789   ->    790\n",
      "gradcam                      787   +    789   ->    790\n",
      "gradcamgithub                787   +    789   ->    790\n",
      "vanillapixel                 787   +    789   ->    790\n",
      "vanillaembedding             787   +    789   ->    790\n",
      "sgpixel                      787   +    789   ->    790\n",
      "sgembedding                  787   +    789   ->    790\n",
      "vargradpixel                 787   +    789   ->    790\n",
      "vargradembedding             787   +    789   ->    790\n",
      "igpixel                      787   +    789   ->    790\n",
      "igembedding                  787   +    789   ->    790\n",
      "leaveoneoutclassifier        787   +    789   ->    790\n",
      "riseclassifier               787   +    789   ->    790\n",
      "ours                         787   +    789   ->    790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|                       | 803/1000 [6:43:04<11:48:49, 215.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       792   +    795   ->    796\n",
      "attention_rollout            791   +    794   ->    795\n",
      "attention_last               791   +    794   ->    795\n",
      "LRP                          791   +    794   ->    795\n",
      "gradcam                      791   +    794   ->    795\n",
      "gradcamgithub                791   +    794   ->    795\n",
      "vanillapixel                 791   +    794   ->    795\n",
      "vanillaembedding             791   +    794   ->    795\n",
      "sgpixel                      791   +    794   ->    795\n",
      "sgembedding                  791   +    794   ->    795\n",
      "vargradpixel                 791   +    794   ->    795\n",
      "vargradembedding             791   +    794   ->    795\n",
      "igpixel                      791   +    794   ->    795\n",
      "igembedding                  791   +    794   ->    795\n",
      "leaveoneoutclassifier        791   +    794   ->    795\n",
      "riseclassifier               791   +    794   ->    795\n",
      "ours                         791   +    794   ->    795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|                       | 807/1000 [6:57:29<11:34:46, 215.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       797   +    800   ->    801\n",
      "attention_rollout            796   +    799   ->    800\n",
      "attention_last               796   +    799   ->    800\n",
      "LRP                          796   +    799   ->    800\n",
      "gradcam                      796   +    799   ->    800\n",
      "gradcamgithub                796   +    799   ->    800\n",
      "vanillapixel                 796   +    799   ->    800\n",
      "vanillaembedding             796   +    799   ->    800\n",
      "sgpixel                      796   +    799   ->    800\n",
      "sgembedding                  796   +    799   ->    800\n",
      "vargradpixel                 796   +    799   ->    800\n",
      "vargradembedding             796   +    799   ->    800\n",
      "igpixel                      796   +    799   ->    800\n",
      "igembedding                  796   +    799   ->    800\n",
      "leaveoneoutclassifier        796   +    799   ->    800\n",
      "riseclassifier               796   +    799   ->    800\n",
      "ours                         796   +    799   ->    800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|                      | 811/1000 [7:11:52<11:20:12, 215.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       802   +    804   ->    805\n",
      "attention_rollout            801   +    803   ->    804\n",
      "attention_last               801   +    803   ->    804\n",
      "LRP                          801   +    803   ->    804\n",
      "gradcam                      801   +    803   ->    804\n",
      "gradcamgithub                801   +    803   ->    804\n",
      "vanillapixel                 801   +    803   ->    804\n",
      "vanillaembedding             801   +    803   ->    804\n",
      "sgpixel                      801   +    803   ->    804\n",
      "sgembedding                  801   +    803   ->    804\n",
      "vargradpixel                 801   +    803   ->    804\n",
      "vargradembedding             801   +    803   ->    804\n",
      "igpixel                      801   +    803   ->    804\n",
      "igembedding                  801   +    803   ->    804\n",
      "leaveoneoutclassifier        801   +    803   ->    804\n",
      "riseclassifier               801   +    803   ->    804\n",
      "ours                         801   +    803   ->    804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|                      | 815/1000 [7:26:15<11:05:32, 215.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       806   +    808   ->    809\n",
      "attention_rollout            805   +    807   ->    808\n",
      "attention_last               805   +    807   ->    808\n",
      "LRP                          805   +    807   ->    808\n",
      "gradcam                      805   +    807   ->    808\n",
      "gradcamgithub                805   +    807   ->    808\n",
      "vanillapixel                 805   +    807   ->    808\n",
      "vanillaembedding             805   +    807   ->    808\n",
      "sgpixel                      805   +    807   ->    808\n",
      "sgembedding                  805   +    807   ->    808\n",
      "vargradpixel                 805   +    807   ->    808\n",
      "vargradembedding             805   +    807   ->    808\n",
      "igpixel                      805   +    807   ->    808\n",
      "igembedding                  805   +    807   ->    808\n",
      "leaveoneoutclassifier        805   +    807   ->    808\n",
      "riseclassifier               805   +    807   ->    808\n",
      "ours                         805   +    807   ->    808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|                     | 819/1000 [7:40:37<10:50:59, 215.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       810   +    812   ->    813\n",
      "attention_rollout            809   +    811   ->    812\n",
      "attention_last               809   +    811   ->    812\n",
      "LRP                          809   +    811   ->    812\n",
      "gradcam                      809   +    811   ->    812\n",
      "gradcamgithub                809   +    811   ->    812\n",
      "vanillapixel                 809   +    811   ->    812\n",
      "vanillaembedding             809   +    811   ->    812\n",
      "sgpixel                      809   +    811   ->    812\n",
      "sgembedding                  809   +    811   ->    812\n",
      "vargradpixel                 809   +    811   ->    812\n",
      "vargradembedding             809   +    811   ->    812\n",
      "igpixel                      809   +    811   ->    812\n",
      "igembedding                  809   +    811   ->    812\n",
      "leaveoneoutclassifier        809   +    811   ->    812\n",
      "riseclassifier               809   +    811   ->    812\n",
      "ours                         809   +    811   ->    812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|                     | 823/1000 [7:55:00<10:36:25, 215.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       814   +    816   ->    817\n",
      "attention_rollout            813   +    815   ->    816\n",
      "attention_last               813   +    815   ->    816\n",
      "LRP                          813   +    815   ->    816\n",
      "gradcam                      813   +    815   ->    816\n",
      "gradcamgithub                813   +    815   ->    816\n",
      "vanillapixel                 813   +    815   ->    816\n",
      "vanillaembedding             813   +    815   ->    816\n",
      "sgpixel                      813   +    815   ->    816\n",
      "sgembedding                  813   +    815   ->    816\n",
      "vargradpixel                 813   +    815   ->    816\n",
      "vargradembedding             813   +    815   ->    816\n",
      "igpixel                      813   +    815   ->    816\n",
      "igembedding                  813   +    815   ->    816\n",
      "leaveoneoutclassifier        813   +    815   ->    816\n",
      "riseclassifier               813   +    815   ->    816\n",
      "ours                         813   +    815   ->    816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|                    | 827/1000 [8:09:22<10:21:50, 215.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       818   +    820   ->    821\n",
      "attention_rollout            817   +    819   ->    820\n",
      "attention_last               817   +    819   ->    820\n",
      "LRP                          817   +    819   ->    820\n",
      "gradcam                      817   +    819   ->    820\n",
      "gradcamgithub                817   +    819   ->    820\n",
      "vanillapixel                 817   +    819   ->    820\n",
      "vanillaembedding             817   +    819   ->    820\n",
      "sgpixel                      817   +    819   ->    820\n",
      "sgembedding                  817   +    819   ->    820\n",
      "vargradpixel                 817   +    819   ->    820\n",
      "vargradembedding             817   +    819   ->    820\n",
      "igpixel                      817   +    819   ->    820\n",
      "igembedding                  817   +    819   ->    820\n",
      "leaveoneoutclassifier        817   +    819   ->    820\n",
      "riseclassifier               817   +    819   ->    820\n",
      "ours                         817   +    819   ->    820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|                    | 831/1000 [8:23:44<10:07:28, 215.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       822   +    824   ->    825\n",
      "attention_rollout            821   +    823   ->    824\n",
      "attention_last               821   +    823   ->    824\n",
      "LRP                          821   +    823   ->    824\n",
      "gradcam                      821   +    823   ->    824\n",
      "gradcamgithub                821   +    823   ->    824\n",
      "vanillapixel                 821   +    823   ->    824\n",
      "vanillaembedding             821   +    823   ->    824\n",
      "sgpixel                      821   +    823   ->    824\n",
      "sgembedding                  821   +    823   ->    824\n",
      "vargradpixel                 821   +    823   ->    824\n",
      "vargradembedding             821   +    823   ->    824\n",
      "igpixel                      821   +    823   ->    824\n",
      "igembedding                  821   +    823   ->    824\n",
      "leaveoneoutclassifier        821   +    823   ->    824\n",
      "riseclassifier               821   +    823   ->    824\n",
      "ours                         821   +    823   ->    824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|                    | 835/1000 [8:38:06<9:52:46, 215.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       826   +    828   ->    829\n",
      "attention_rollout            825   +    827   ->    828\n",
      "attention_last               825   +    827   ->    828\n",
      "LRP                          825   +    827   ->    828\n",
      "gradcam                      825   +    827   ->    828\n",
      "gradcamgithub                825   +    827   ->    828\n",
      "vanillapixel                 825   +    827   ->    828\n",
      "vanillaembedding             825   +    827   ->    828\n",
      "sgpixel                      825   +    827   ->    828\n",
      "sgembedding                  825   +    827   ->    828\n",
      "vargradpixel                 825   +    827   ->    828\n",
      "vargradembedding             825   +    827   ->    828\n",
      "igpixel                      825   +    827   ->    828\n",
      "igembedding                  825   +    827   ->    828\n",
      "leaveoneoutclassifier        825   +    827   ->    828\n",
      "riseclassifier               825   +    827   ->    828\n",
      "ours                         825   +    827   ->    828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|                   | 839/1000 [8:52:30<9:38:54, 215.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       830   +    832   ->    833\n",
      "attention_rollout            829   +    831   ->    832\n",
      "attention_last               829   +    831   ->    832\n",
      "LRP                          829   +    831   ->    832\n",
      "gradcam                      829   +    831   ->    832\n",
      "gradcamgithub                829   +    831   ->    832\n",
      "vanillapixel                 829   +    831   ->    832\n",
      "vanillaembedding             829   +    831   ->    832\n",
      "sgpixel                      829   +    831   ->    832\n",
      "sgembedding                  829   +    831   ->    832\n",
      "vargradpixel                 829   +    831   ->    832\n",
      "vargradembedding             829   +    831   ->    832\n",
      "igpixel                      829   +    831   ->    832\n",
      "igembedding                  829   +    831   ->    832\n",
      "leaveoneoutclassifier        829   +    831   ->    832\n",
      "riseclassifier               829   +    831   ->    832\n",
      "ours                         829   +    831   ->    832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|                   | 843/1000 [9:06:54<9:24:36, 215.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       834   +    836   ->    837\n",
      "attention_rollout            833   +    835   ->    836\n",
      "attention_last               833   +    835   ->    836\n",
      "LRP                          833   +    835   ->    836\n",
      "gradcam                      833   +    835   ->    836\n",
      "gradcamgithub                833   +    835   ->    836\n",
      "vanillapixel                 833   +    835   ->    836\n",
      "vanillaembedding             833   +    835   ->    836\n",
      "sgpixel                      833   +    835   ->    836\n",
      "sgembedding                  833   +    835   ->    836\n",
      "vargradpixel                 833   +    835   ->    836\n",
      "vargradembedding             833   +    835   ->    836\n",
      "igpixel                      833   +    835   ->    836\n",
      "igembedding                  833   +    835   ->    836\n",
      "leaveoneoutclassifier        833   +    835   ->    836\n",
      "riseclassifier               833   +    835   ->    836\n",
      "ours                         833   +    835   ->    836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|                  | 847/1000 [9:21:16<9:10:05, 215.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       838   +    840   ->    841\n",
      "attention_rollout            837   +    839   ->    840\n",
      "attention_last               837   +    839   ->    840\n",
      "LRP                          837   +    839   ->    840\n",
      "gradcam                      837   +    839   ->    840\n",
      "gradcamgithub                837   +    839   ->    840\n",
      "vanillapixel                 837   +    839   ->    840\n",
      "vanillaembedding             837   +    839   ->    840\n",
      "sgpixel                      837   +    839   ->    840\n",
      "sgembedding                  837   +    839   ->    840\n",
      "vargradpixel                 837   +    839   ->    840\n",
      "vargradembedding             837   +    839   ->    840\n",
      "igpixel                      837   +    839   ->    840\n",
      "igembedding                  837   +    839   ->    840\n",
      "leaveoneoutclassifier        837   +    839   ->    840\n",
      "riseclassifier               837   +    839   ->    840\n",
      "ours                         837   +    839   ->    840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|                  | 851/1000 [9:35:39<8:55:45, 215.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       842   +    844   ->    845\n",
      "attention_rollout            841   +    843   ->    844\n",
      "attention_last               841   +    843   ->    844\n",
      "LRP                          841   +    843   ->    844\n",
      "gradcam                      841   +    843   ->    844\n",
      "gradcamgithub                841   +    843   ->    844\n",
      "vanillapixel                 841   +    843   ->    844\n",
      "vanillaembedding             841   +    843   ->    844\n",
      "sgpixel                      841   +    843   ->    844\n",
      "sgembedding                  841   +    843   ->    844\n",
      "vargradpixel                 841   +    843   ->    844\n",
      "vargradembedding             841   +    843   ->    844\n",
      "igpixel                      841   +    843   ->    844\n",
      "igembedding                  841   +    843   ->    844\n",
      "leaveoneoutclassifier        841   +    843   ->    844\n",
      "riseclassifier               841   +    843   ->    844\n",
      "ours                         841   +    843   ->    844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|                 | 855/1000 [9:50:01<8:41:11, 215.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       846   +    848   ->    849\n",
      "attention_rollout            845   +    847   ->    848\n",
      "attention_last               845   +    847   ->    848\n",
      "LRP                          845   +    847   ->    848\n",
      "gradcam                      845   +    847   ->    848\n",
      "gradcamgithub                845   +    847   ->    848\n",
      "vanillapixel                 845   +    847   ->    848\n",
      "vanillaembedding             845   +    847   ->    848\n",
      "sgpixel                      845   +    847   ->    848\n",
      "sgembedding                  845   +    847   ->    848\n",
      "vargradpixel                 845   +    847   ->    848\n",
      "vargradembedding             845   +    847   ->    848\n",
      "igpixel                      845   +    847   ->    848\n",
      "igembedding                  845   +    847   ->    848\n",
      "leaveoneoutclassifier        845   +    847   ->    848\n",
      "riseclassifier               845   +    847   ->    848\n",
      "ours                         845   +    847   ->    848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|                 | 859/1000 [10:04:23<8:26:43, 215.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       850   +    852   ->    853\n",
      "attention_rollout            849   +    851   ->    852\n",
      "attention_last               849   +    851   ->    852\n",
      "LRP                          849   +    851   ->    852\n",
      "gradcam                      849   +    851   ->    852\n",
      "gradcamgithub                849   +    851   ->    852\n",
      "vanillapixel                 849   +    851   ->    852\n",
      "vanillaembedding             849   +    851   ->    852\n",
      "sgpixel                      849   +    851   ->    852\n",
      "sgembedding                  849   +    851   ->    852\n",
      "vargradpixel                 849   +    851   ->    852\n",
      "vargradembedding             849   +    851   ->    852\n",
      "igpixel                      849   +    851   ->    852\n",
      "igembedding                  849   +    851   ->    852\n",
      "leaveoneoutclassifier        849   +    851   ->    852\n",
      "riseclassifier               849   +    851   ->    852\n",
      "ours                         849   +    851   ->    852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|                | 863/1000 [10:18:45<8:12:16, 215.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       854   +    856   ->    857\n",
      "attention_rollout            853   +    855   ->    856\n",
      "attention_last               853   +    855   ->    856\n",
      "LRP                          853   +    855   ->    856\n",
      "gradcam                      853   +    855   ->    856\n",
      "gradcamgithub                853   +    855   ->    856\n",
      "vanillapixel                 853   +    855   ->    856\n",
      "vanillaembedding             853   +    855   ->    856\n",
      "sgpixel                      853   +    855   ->    856\n",
      "sgembedding                  853   +    855   ->    856\n",
      "vargradpixel                 853   +    855   ->    856\n",
      "vargradembedding             853   +    855   ->    856\n",
      "igpixel                      853   +    855   ->    856\n",
      "igembedding                  853   +    855   ->    856\n",
      "leaveoneoutclassifier        853   +    855   ->    856\n",
      "riseclassifier               853   +    855   ->    856\n",
      "ours                         853   +    855   ->    856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|                | 867/1000 [10:33:07<7:57:49, 215.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       858   +    860   ->    861\n",
      "attention_rollout            857   +    859   ->    860\n",
      "attention_last               857   +    859   ->    860\n",
      "LRP                          857   +    859   ->    860\n",
      "gradcam                      857   +    859   ->    860\n",
      "gradcamgithub                857   +    859   ->    860\n",
      "vanillapixel                 857   +    859   ->    860\n",
      "vanillaembedding             857   +    859   ->    860\n",
      "sgpixel                      857   +    859   ->    860\n",
      "sgembedding                  857   +    859   ->    860\n",
      "vargradpixel                 857   +    859   ->    860\n",
      "vargradembedding             857   +    859   ->    860\n",
      "igpixel                      857   +    859   ->    860\n",
      "igembedding                  857   +    859   ->    860\n",
      "leaveoneoutclassifier        857   +    859   ->    860\n",
      "riseclassifier               857   +    859   ->    860\n",
      "ours                         857   +    859   ->    860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|               | 871/1000 [10:47:32<7:43:50, 215.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       862   +    864   ->    865\n",
      "attention_rollout            861   +    863   ->    864\n",
      "attention_last               861   +    863   ->    864\n",
      "LRP                          861   +    863   ->    864\n",
      "gradcam                      861   +    863   ->    864\n",
      "gradcamgithub                861   +    863   ->    864\n",
      "vanillapixel                 861   +    863   ->    864\n",
      "vanillaembedding             861   +    863   ->    864\n",
      "sgpixel                      861   +    863   ->    864\n",
      "sgembedding                  861   +    863   ->    864\n",
      "vargradpixel                 861   +    863   ->    864\n",
      "vargradembedding             861   +    863   ->    864\n",
      "igpixel                      861   +    863   ->    864\n",
      "igembedding                  861   +    863   ->    864\n",
      "leaveoneoutclassifier        861   +    863   ->    864\n",
      "riseclassifier               861   +    863   ->    864\n",
      "ours                         861   +    863   ->    864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|               | 875/1000 [11:01:55<7:29:28, 215.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       866   +    869   ->    870\n",
      "attention_rollout            865   +    868   ->    869\n",
      "attention_last               865   +    868   ->    869\n",
      "LRP                          865   +    868   ->    869\n",
      "gradcam                      865   +    868   ->    869\n",
      "gradcamgithub                865   +    868   ->    869\n",
      "vanillapixel                 865   +    868   ->    869\n",
      "vanillaembedding             865   +    868   ->    869\n",
      "sgpixel                      865   +    868   ->    869\n",
      "sgembedding                  865   +    868   ->    869\n",
      "vargradpixel                 865   +    868   ->    869\n",
      "vargradembedding             865   +    868   ->    869\n",
      "igpixel                      865   +    868   ->    869\n",
      "igembedding                  865   +    868   ->    869\n",
      "leaveoneoutclassifier        865   +    868   ->    869\n",
      "riseclassifier               865   +    868   ->    869\n",
      "ours                         865   +    868   ->    869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|              | 879/1000 [11:16:16<7:14:47, 215.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       871   +    873   ->    874\n",
      "attention_rollout            870   +    872   ->    873\n",
      "attention_last               870   +    872   ->    873\n",
      "LRP                          870   +    872   ->    873\n",
      "gradcam                      870   +    872   ->    873\n",
      "gradcamgithub                870   +    872   ->    873\n",
      "vanillapixel                 870   +    872   ->    873\n",
      "vanillaembedding             870   +    872   ->    873\n",
      "sgpixel                      870   +    872   ->    873\n",
      "sgembedding                  870   +    872   ->    873\n",
      "vargradpixel                 870   +    872   ->    873\n",
      "vargradembedding             870   +    872   ->    873\n",
      "igpixel                      870   +    872   ->    873\n",
      "igembedding                  870   +    872   ->    873\n",
      "leaveoneoutclassifier        870   +    872   ->    873\n",
      "riseclassifier               870   +    872   ->    873\n",
      "ours                         870   +    872   ->    873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|              | 883/1000 [11:30:37<7:00:12, 215.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       875   +    877   ->    878\n",
      "attention_rollout            874   +    876   ->    877\n",
      "attention_last               874   +    876   ->    877\n",
      "LRP                          874   +    876   ->    877\n",
      "gradcam                      874   +    876   ->    877\n",
      "gradcamgithub                874   +    876   ->    877\n",
      "vanillapixel                 874   +    876   ->    877\n",
      "vanillaembedding             874   +    876   ->    877\n",
      "sgpixel                      874   +    876   ->    877\n",
      "sgembedding                  874   +    876   ->    877\n",
      "vargradpixel                 874   +    876   ->    877\n",
      "vargradembedding             874   +    876   ->    877\n",
      "igpixel                      874   +    876   ->    877\n",
      "igembedding                  874   +    876   ->    877\n",
      "leaveoneoutclassifier        874   +    876   ->    877\n",
      "riseclassifier               874   +    876   ->    877\n",
      "ours                         874   +    876   ->    877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|             | 887/1000 [11:44:59<6:45:53, 215.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       879   +    881   ->    882\n",
      "attention_rollout            878   +    880   ->    881\n",
      "attention_last               878   +    880   ->    881\n",
      "LRP                          878   +    880   ->    881\n",
      "gradcam                      878   +    880   ->    881\n",
      "gradcamgithub                878   +    880   ->    881\n",
      "vanillapixel                 878   +    880   ->    881\n",
      "vanillaembedding             878   +    880   ->    881\n",
      "sgpixel                      878   +    880   ->    881\n",
      "sgembedding                  878   +    880   ->    881\n",
      "vargradpixel                 878   +    880   ->    881\n",
      "vargradembedding             878   +    880   ->    881\n",
      "igpixel                      878   +    880   ->    881\n",
      "igembedding                  878   +    880   ->    881\n",
      "leaveoneoutclassifier        878   +    880   ->    881\n",
      "riseclassifier               878   +    880   ->    881\n",
      "ours                         878   +    880   ->    881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|             | 891/1000 [11:59:22<6:31:39, 215.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       883   +    885   ->    886\n",
      "attention_rollout            882   +    884   ->    885\n",
      "attention_last               882   +    884   ->    885\n",
      "LRP                          882   +    884   ->    885\n",
      "gradcam                      882   +    884   ->    885\n",
      "gradcamgithub                882   +    884   ->    885\n",
      "vanillapixel                 882   +    884   ->    885\n",
      "vanillaembedding             882   +    884   ->    885\n",
      "sgpixel                      882   +    884   ->    885\n",
      "sgembedding                  882   +    884   ->    885\n",
      "vargradpixel                 882   +    884   ->    885\n",
      "vargradembedding             882   +    884   ->    885\n",
      "igpixel                      882   +    884   ->    885\n",
      "igembedding                  882   +    884   ->    885\n",
      "leaveoneoutclassifier        882   +    884   ->    885\n",
      "riseclassifier               882   +    884   ->    885\n",
      "ours                         882   +    884   ->    885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|            | 895/1000 [12:13:45<6:17:16, 215.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       887   +    889   ->    890\n",
      "attention_rollout            886   +    888   ->    889\n",
      "attention_last               886   +    888   ->    889\n",
      "LRP                          886   +    888   ->    889\n",
      "gradcam                      886   +    888   ->    889\n",
      "gradcamgithub                886   +    888   ->    889\n",
      "vanillapixel                 886   +    888   ->    889\n",
      "vanillaembedding             886   +    888   ->    889\n",
      "sgpixel                      886   +    888   ->    889\n",
      "sgembedding                  886   +    888   ->    889\n",
      "vargradpixel                 886   +    888   ->    889\n",
      "vargradembedding             886   +    888   ->    889\n",
      "igpixel                      886   +    888   ->    889\n",
      "igembedding                  886   +    888   ->    889\n",
      "leaveoneoutclassifier        886   +    888   ->    889\n",
      "riseclassifier               886   +    888   ->    889\n",
      "ours                         886   +    888   ->    889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|            | 899/1000 [12:28:05<6:02:36, 215.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       891   +    893   ->    894\n",
      "attention_rollout            890   +    892   ->    893\n",
      "attention_last               890   +    892   ->    893\n",
      "LRP                          890   +    892   ->    893\n",
      "gradcam                      890   +    892   ->    893\n",
      "gradcamgithub                890   +    892   ->    893\n",
      "vanillapixel                 890   +    892   ->    893\n",
      "vanillaembedding             890   +    892   ->    893\n",
      "sgpixel                      890   +    892   ->    893\n",
      "sgembedding                  890   +    892   ->    893\n",
      "vargradpixel                 890   +    892   ->    893\n",
      "vargradembedding             890   +    892   ->    893\n",
      "igpixel                      890   +    892   ->    893\n",
      "igembedding                  890   +    892   ->    893\n",
      "leaveoneoutclassifier        890   +    892   ->    893\n",
      "riseclassifier               890   +    892   ->    893\n",
      "ours                         890   +    892   ->    893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|           | 903/1000 [12:42:26<5:48:14, 215.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       895   +    897   ->    898\n",
      "attention_rollout            894   +    896   ->    897\n",
      "attention_last               894   +    896   ->    897\n",
      "LRP                          894   +    896   ->    897\n",
      "gradcam                      894   +    896   ->    897\n",
      "gradcamgithub                894   +    896   ->    897\n",
      "vanillapixel                 894   +    896   ->    897\n",
      "vanillaembedding             894   +    896   ->    897\n",
      "sgpixel                      894   +    896   ->    897\n",
      "sgembedding                  894   +    896   ->    897\n",
      "vargradpixel                 894   +    896   ->    897\n",
      "vargradembedding             894   +    896   ->    897\n",
      "igpixel                      894   +    896   ->    897\n",
      "igembedding                  894   +    896   ->    897\n",
      "leaveoneoutclassifier        894   +    896   ->    897\n",
      "riseclassifier               894   +    896   ->    897\n",
      "ours                         894   +    896   ->    897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|           | 907/1000 [12:56:47<5:33:48, 215.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|           | 910/1000 [13:11:03<1:18:14, 52.16s/it]\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(insertdelete_save_dict_path):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(insertdelete_save_dict_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 128\u001b[0m         insertdelete_save_dict_loaded\u001b[38;5;241m=\u001b[39m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     insertdelete_save_dict_loaded\u001b[38;5;241m=\u001b[39m{}\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "if evaluation_stage==\"4_insert_delete\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" and (len(explanation_method_to_run)!=1 or explanation_method_to_run[0]!=\"kernelshap\") else None)):\n",
    "        if dataset_split==\"test\" and (len(explanation_method_to_run)!=1 or explanation_method_to_run[0]!=\"kernelshap\"):\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=insertdelete_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if explanation_method=='kernelshap':\n",
    "                    if all([(path in data_keys) or (path not in estimationerror_sample_path_list) for path in paths]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(explanation_method,'not exist')\n",
    "                        updated_signal_list.append(explanation_method)                \n",
    "                else:\n",
    "                    if all([path in data_keys for path in paths]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(explanation_method,'not exist')\n",
    "                        updated_signal_list.append(explanation_method)                      \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                \n",
    "                insertdelete_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation in zip(images, explanations):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_players+1, num_classes)\n",
    "                            insertdelete_dict[metric_mode].append(prob)\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # ( , num_players)\n",
    "                            insertdelete_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for class_idx in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[class_idx]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))                                  \n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob[:, class_idx])\n",
    "                            prob=np.array(prob_)\n",
    "                            insertdelete_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                            \n",
    "                if explanation_method==\"random\":\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "                \n",
    "                if explanation_method not in updated_signal_list:\n",
    "                    continue                \n",
    "                \n",
    "                insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                if os.path.isfile(insertdelete_save_dict_path):\n",
    "                    with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                        insertdelete_save_dict_loaded=pickle.load(f)\n",
    "                else:\n",
    "                    insertdelete_save_dict_loaded={}\n",
    "\n",
    "                len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "                len_loaded=len(insertdelete_save_dict_loaded)\n",
    "                insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "                len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "\n",
    "                print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                with open(insertdelete_save_dict_path, \"wb\") as f:\n",
    "                    pickle.dump(insertdelete_save_dict_backbone_method, f)           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecfa8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                       899   +    913   ->    914\n",
      "attention_rollout            898   +    912   ->    913\n",
      "attention_last               898   +    912   ->    913\n",
      "LRP                          898   +    912   ->    913\n",
      "gradcam                      898   +    912   ->    913\n",
      "gradcamgithub                898   +    912   ->    913\n",
      "vanillapixel                 898   +    912   ->    913\n",
      "vanillaembedding             898   +    912   ->    913\n",
      "sgpixel                      898   +    912   ->    913\n",
      "sgembedding                  898   +    912   ->    913\n",
      "vargradpixel                 898   +    912   ->    913\n",
      "vargradembedding             898   +    912   ->    913\n",
      "igpixel                      898   +    912   ->    913\n",
      "igembedding                  898   +    912   ->    913\n",
      "leaveoneoutclassifier        898   +    912   ->    913\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier               898   +    912   ->    913\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                         898   +    912   ->    913\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "if evaluation_stage==\"4_insert_delete\":\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "            insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(insertdelete_save_dict_path):\n",
    "                with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                    insertdelete_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                insertdelete_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "            len_loaded=len(insertdelete_save_dict_loaded)\n",
    "            insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "            len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "            with open(insertdelete_save_dict_path, \"wb\") as f:\n",
    "                pickle.dump(insertdelete_save_dict_backbone_method, f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f433b",
   "metadata": {},
   "source": [
    "# 5_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(num_players: int, num_mask_samples: int or None = None, paired_mask_samples: bool = True,\n",
    "                  mode: str = 'uniform', random_state: np.random.RandomState or None = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_players: the number of players in the coalitional game\n",
    "        num_mask_samples: the number of masks to generate\n",
    "        paired_mask_samples: if True, the generated masks are pairs of x and 1-x.\n",
    "        mode: the distribution that the number of masked features follows. ('uniform' or 'shapley')\n",
    "        random_state: random generator\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape\n",
    "        (num_masks, num_players) if num_masks is int\n",
    "        (num_players) if num_masks is None\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = random_state or np.random\n",
    "\n",
    "    num_samples_ = num_mask_samples or 1\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        assert num_samples_ % 2 == 0, \"'num_samples' must be a multiple of 2 if 'paired' is True\"\n",
    "        num_samples_ = num_samples_ // 2\n",
    "    else:\n",
    "        num_samples_ = num_samples_\n",
    "\n",
    "    if mode == 'uniform':\n",
    "        masks = (random_state.rand(num_samples_, num_players) > random_state.rand(num_samples_, 1)).astype('int')\n",
    "    elif mode == 'shapley':\n",
    "        probs = 1 / (np.arange(1, num_players) * (num_players - np.arange(1, num_players)))\n",
    "        probs = probs / probs.sum()\n",
    "        masks = (random_state.rand(num_samples_, num_players) > 1 / num_players * random_state.choice(\n",
    "            np.arange(num_players - 1), p=probs, size=[num_samples_, 1])).astype('int')\n",
    "    else:\n",
    "        raise ValueError(\"'mode' must be 'random' or 'shapley'\")\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        masks = np.stack([masks, 1 - masks], axis=1).reshape(num_samples_ * 2, num_players)\n",
    "\n",
    "    if num_mask_samples is None:\n",
    "        masks = masks.squeeze(0)\n",
    "        return masks  # (num_masks)\n",
    "    else:\n",
    "        return masks  # (num_samples, num_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4bf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"5_sensitivity\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):            \n",
    "            for image, path in zip(images, paths):\n",
    "                #path=path.replace('l0.cs.washington.edu','l2lambda.cs.washington.edu')\n",
    "                for num_included_players in [\"all\"] + list(range(14, 196, 14)):\n",
    "                    if num_included_players==\"all\":\n",
    "                        prob_all=[]\n",
    "                        mask_all=[]\n",
    "                        for random_iter in range(20):\n",
    "                            image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                            mask=generate_mask(num_players=num_players,\n",
    "                                               num_mask_samples=50,\n",
    "                                               paired_mask_samples=False,\n",
    "                                               mode=\"uniform\",\n",
    "                                               random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                            prob_all.append(prob)\n",
    "                            mask_all.append(mask)\n",
    "                        prob_all=np.concatenate(prob_all, axis=0)\n",
    "                        mask_all=np.concatenate(mask_all, axis=0)\n",
    "                    else:\n",
    "                        prob_all=[]\n",
    "                        mask_all=[]\n",
    "                        for random_iter in range(20):\n",
    "                            image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)                                                        \n",
    "                            mask=np.zeros((50, num_players))\n",
    "                            mask[:, :num_included_players]=1\n",
    "                            for i in range(len(mask)):\n",
    "                                mask[i]=np.random.RandomState(42+10*random_iter+i).permutation(mask[i])\n",
    "\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                            prob_all.append(prob)\n",
    "                            mask_all.append(mask)\n",
    "                        prob_all=np.concatenate(prob_all, axis=0)\n",
    "                        mask_all=np.concatenate(mask_all, axis=0)\n",
    "                    \n",
    "                    for explanation_method in explanation_method_to_run:                \n",
    "                        if explanation_method==\"random\":\n",
    "                            continue\n",
    "                        explanation=explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys())[0])]['explanation']\n",
    "                        explanation=explanation+np.random.RandomState(42).uniform(low=0, high=1e-40, size=explanation.shape)\n",
    "                        explanation_mask=explanation@(mask_all.T)\n",
    "\n",
    "                        if len(explanation.shape)==1:\n",
    "                            correlation = np.array([stats.spearmanr(explanation_mask, prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                            assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                            \n",
    "                        elif len(explanation.shape)==2:\n",
    "                            #correlation=stats.spearmanr(np.concatenate([explanation_mask, prob_all.T], axis=0), axis=1).correlation\n",
    "                            correlation = np.array([stats.spearmanr(explanation_mask[i], prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                            assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                        else:\n",
    "                            raise\n",
    "                        sensitivity_save_dit_update(backbone_type, explanation_method,\n",
    "                                                     num_included_players=num_included_players,\n",
    "                                                     path_list=[path], sensitivity_list=[correlation],\n",
    "                                                     shape=(_config[\"output_dim\"],))     \n",
    "                    \n",
    "                    \n",
    "                \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            for explanation_method, sensitivity_save_dit_backbone_method in sensitivity_save_dit[backbone_type].items():\n",
    "                sensitivity_save_dit_path=f'results/5_sensitivity/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                if os.path.isfile(sensitivity_save_dit_path):\n",
    "                    with open(sensitivity_save_dit_path, 'rb') as f:\n",
    "                        sensitivity_save_dit_loaded=pickle.load(f)\n",
    "                else:\n",
    "                    sensitivity_save_dit_loaded={}\n",
    "\n",
    "                len_original=len(sensitivity_save_dit_backbone_method)            \n",
    "                len_loaded=len(sensitivity_save_dit_loaded)\n",
    "                sensitivity_save_dit_backbone_method.update(sensitivity_save_dit_loaded)\n",
    "                len_updated=len(sensitivity_save_dit_backbone_method)\n",
    "\n",
    "                print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                with open(sensitivity_save_dit_path, \"wb\") as f:\n",
    "                    pickle.dump(sensitivity_save_dit_backbone_method, f)        \n",
    "                    \n",
    "             \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89881f09",
   "metadata": {},
   "source": [
    "# 6_noretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca77a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"6_noretraining\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in [\"random\"]+explanation_method_to_run:\n",
    "                data_keys=noretraining_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)                \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                noretraining_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation, label in zip(images, explanations, labels):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_classes, num_players+1)\n",
    "                            noretraining_dict[metric_mode].append(prob)#(prob.argmax(axis=1)==label.item()).astype(float))\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # (num_classes, num_players+1)                            \n",
    "                            #noretraining_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float)) \n",
    "                            noretraining_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            #mask=explanation_to_mask(explanation=explanation[label.item()], mode=metric_mode)\n",
    "                            if len(explanation)==1:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[0])[np.newaxis,:], mode=metric_mode)\n",
    "                            else:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[label.item()])[np.newaxis,:], mode=metric_mode)\n",
    "                            \n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T\n",
    "                            #noretraining_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float))\n",
    "                            noretraining_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                          \n",
    "                if explanation_method==\"random\":\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:        \n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, noretraining_save_dict_backbone_method in noretraining_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue                \n",
    "\n",
    "                    noretraining_save_dict_path=f'results/6_noretraining/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(noretraining_save_dict_path):\n",
    "                        try:\n",
    "                            with open(noretraining_save_dict_path, 'rb') as f:\n",
    "                                noretraining_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            noretraining_save_dict_loaded={}\n",
    "                    else:\n",
    "                        noretraining_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(noretraining_save_dict_backbone_method)            \n",
    "                    len_loaded=len(noretraining_save_dict_loaded)\n",
    "                    noretraining_save_dict_backbone_method.update(noretraining_save_dict_loaded)\n",
    "                    len_updated=len(noretraining_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                    with open(noretraining_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(noretraining_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f74b5",
   "metadata": {},
   "source": [
    "# 7_classifiermasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"7_classifiermasked\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in [\"random\"]+explanation_method_to_run:\n",
    "                data_keys=classifiermasked_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)                \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                classifiermasked_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation, label in zip(images, explanations, labels):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(classifier_masked_dict[backbone_type].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                classifier_masked_dict[backbone_type].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(classifier_masked_dict[backbone_type].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_classes, num_players+1)\n",
    "                            classifiermasked_dict[metric_mode].append(prob)#(prob.argmax(axis=1)==label.item()).astype(float))\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            classifier_masked_dict[backbone_type].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(classifier_masked_dict[backbone_type].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # (num_classes, num_players+1)                            \n",
    "                            #classifiermasked_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float)) \n",
    "                            classifiermasked_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            #mask=explanation_to_mask(explanation=explanation[label.item()], mode=metric_mode)\n",
    "                            if len(explanation)==1:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[0])[np.newaxis,:], mode=metric_mode)\n",
    "                            else:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[label.item()])[np.newaxis,:], mode=metric_mode)\n",
    "                            \n",
    "                            classifier_masked_dict[backbone_type].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(classifier_masked_dict[backbone_type].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T\n",
    "                            #classifiermasked_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float))\n",
    "                            classifiermasked_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                          \n",
    "                if explanation_method==\"random\":\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:        \n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, classifiermasked_save_dict_backbone_method in classifiermasked_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue                \n",
    "\n",
    "                    classifiermasked_save_dict_path=f'results/7_classifiermasked/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(classifiermasked_save_dict_path):\n",
    "                        try:\n",
    "                            with open(classifiermasked_save_dict_path, 'rb') as f:\n",
    "                                classifiermasked_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            classifiermasked_save_dict_loaded={}\n",
    "                    else:\n",
    "                        classifiermasked_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(classifiermasked_save_dict_backbone_method)            \n",
    "                    len_loaded=len(classifiermasked_save_dict_loaded)\n",
    "                    classifiermasked_save_dict_backbone_method.update(classifiermasked_save_dict_loaded)\n",
    "                    len_updated=len(classifiermasked_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                    with open(classifiermasked_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(classifiermasked_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcf947",
   "metadata": {},
   "source": [
    "# 8_elapsedtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e1873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_random_explanation(num_players, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_players,))\n",
    "    else:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_samples, num_players))\n",
    "\n",
    "\n",
    "if evaluation_stage==\"8_elapsedtime\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=elapsedtime_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                    \n",
    "                if explanation_method==\"random\":\n",
    "                    start_time=time.time()\n",
    "                    explanation_random_list=[get_random_explanation(num_players=196) for path in paths]\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'random', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])\n",
    "                \n",
    "                elif explanation_method==\"attention_rollout\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_rollout_list=attentions_to_explanation(attentions, mode='rollout')\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'attention_rollout', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])\n",
    "\n",
    "                elif explanation_method==\"attention_last\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_last_list=attentions_to_explanation(attentions, mode=-1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'attention_last', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])            \n",
    "\n",
    "                elif explanation_method==\"LRP\":\n",
    "                    explanation_lrp_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_lrp_list.append(np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                              original_image=image.squeeze(0),\n",
    "                                                                              class_index=i,\n",
    "                                                                              mode='transformer_attribution').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0))\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'LRP', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"gradcam\":\n",
    "                    explanation_gradcam_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcam = np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                                              original_image=image.squeeze(0),\n",
    "                                                                                              class_index=i,\n",
    "                                                                                              mode='attn_gradcam').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcam = np.nan_to_num(explanation_gradcam,nan=0)+np.random.uniform(low=0, high=1e-20, size=explanation_gradcam.shape)                    \n",
    "                        explanation_gradcam_list.append(explanation_gradcam)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'gradcam', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "\n",
    "                elif explanation_method==\"gradcamgithub\":\n",
    "                    explanation_gradcamgithub_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcamgithub = np.concatenate([cam_dict[backbone_type](input_tensor=image.unsqueeze(0).to(next(cam_dict[backbone_type].model.parameters()).device),\n",
    "                                                                                            targets=[ClassifierOutputTarget(i)], resize=False).flatten()[np.newaxis,:] for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcamgithub_list.append(explanation_gradcamgithub)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'gradcamgithub', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vanillapixel\":\n",
    "                    explanation_vanillapixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_pixel=saliency_pixel_dict[backbone_type])\n",
    "                        explanation_vanillapixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vanillapixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vanillaembedding\":\n",
    "                    explanation_vanillaembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:\n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_embedding=saliency_embedding_dict[backbone_type])\n",
    "                        explanation_vanillaembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vanillaembedding', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"sgpixel\":\n",
    "                    explanation_sgpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_sgpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'sgpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "\n",
    "                elif explanation_method==\"sgembedding\":\n",
    "                    explanation_sgembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_sgembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'sgembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                                \n",
    "\n",
    "                elif explanation_method==\"vargradpixel\":\n",
    "                    explanation_vargradpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_vargradpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vargradpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vargradembedding\":\n",
    "                    explanation_vargradembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_vargradembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vargradembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                                \n",
    "\n",
    "                elif explanation_method==\"igpixel\":\n",
    "                    explanation_igpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_pixel=ig_pixel_dict[backbone_type])\n",
    "                        explanation_igpixel_list.append(grad[\"attributions_pixel_patchsum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'igpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"igembedding\":\n",
    "                    explanation_igembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_embedding=ig_embedding_dict[backbone_type])\n",
    "                        explanation_igembedding_list.append(grad[\"attributions_embedding_sum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'igembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "\n",
    "                elif explanation_method==\"leaveoneoutclassifier\":\n",
    "                    explanation_leaveoneoutclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutclassifier = leave_one_out(classifier=classifier_dict_[backbone_type], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutclassifier_list.append(explanation_leaveoneoutclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'leaveoneoutclassifier', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"leaveoneoutsurrogate\":\n",
    "                    explanation_leaveoneoutsurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutsurrogate = leave_one_out(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutsurrogate_list.append(explanation_leaveoneoutsurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'leaveoneoutsurrogate', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "\n",
    "                elif explanation_method==\"riseclassifier\":\n",
    "                    explanation_riseclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_riseclassifier = rise(classifier=classifier_dict_[backbone_type], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_riseclassifier_list.append(explanation_riseclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'riseclassifier', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"risesurrogate\":\n",
    "                    explanation_risesurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_risesurrogate = rise(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_risesurrogate_list.append(explanation_risesurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'risesurrogate', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "                    \n",
    "                elif explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'ours', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])                                \n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, elapsedtime_save_dict_backbone_method in elapsedtime_save_dict[backbone_type].items():\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "                    elapsedtime_save_dict_path=f'results/8_elapsedtime/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(elapsedtime_save_dict_path):\n",
    "                        try:\n",
    "                            with open(elapsedtime_save_dict_path, 'rb') as f:\n",
    "                                elapsedtime_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            elapsedtime_save_dict_loaded={}\n",
    "                    else:\n",
    "                        elapsedtime_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(elapsedtime_save_dict_backbone_method)            \n",
    "                    len_loaded=len(elapsedtime_save_dict_loaded)\n",
    "                    elapsedtime_save_dict_backbone_method.update(elapsedtime_save_dict_loaded)\n",
    "                    len_updated=len(elapsedtime_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "                    with open(elapsedtime_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(elapsedtime_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe2ef4",
   "metadata": {},
   "source": [
    "# 9_estimationerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3afb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_sample_path_list=pd.DataFrame(data_loader.dataset.data).groupby(\"label\").apply(lambda x: x.sample(n=10, random_state=42))[\"img_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d02b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"9_estimationerror\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "#         if dataset_split==\"test\":\n",
    "#             if idx>int(1000/data_loader.batch_size+0.5):\n",
    "#                 break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in [\"kernelshapnopair\"]:\n",
    "                data_keys=estimationerror_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([(path in data_keys) or (path not in estimationerror_sample_path_list) for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                    \n",
    "                if explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'ours', \n",
    "                                                     path_list=[path for path, paths in zip(paths, paths) if path in estimationerror_sample_path_list],\n",
    "                                                     estimation_list=[estimation for path, estimation in zip(paths, explanation_ours) if path in estimationerror_sample_path_list],\n",
    "                                                     label_list=[label for path, label in zip(paths, labels.cpu().numpy().tolist()) if path in estimationerror_sample_path_list],\n",
    "                                                     shape=(_config[\"output_dim\"], 196))\n",
    "                    \n",
    "                elif explanation_method==\"kernelshap\":\n",
    "                    path_list=[]\n",
    "                    explanation_kernelshap_list=[]\n",
    "                    label_list=[]\n",
    "                    \n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path, image, label in zip(paths, images, labels.cpu().numpy().tolist()):\n",
    "                        if path in estimationerror_sample_path_list:\n",
    "                            start_time=time.time()      \n",
    "                            game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped_dict[backbone_type],\n",
    "                                                                         image)\n",
    "                            \n",
    "                            explanation_kernelshap = shapley.ShapleyRegression(game, \n",
    "                                                                    batch_size=64, \n",
    "                                                                    thresh=0.1,\n",
    "                                                                    variance_batches=60,\n",
    "                                                                    return_all=True)                              \n",
    "                            \n",
    "                            path_list.append(path)\n",
    "                            explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                            label_list.append(label)\n",
    "                            \n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'kernelshap', \n",
    "                                                     path_list=path_list, \n",
    "                                                     estimation_list=explanation_kernelshap_list, \n",
    "                                                     label_list=label_list,\n",
    "                                                     shape=None)  \n",
    "                    \n",
    "                elif explanation_method==\"kernelshapnopair\":\n",
    "                    path_list=[]\n",
    "                    explanation_kernelshap_list=[]\n",
    "                    label_list=[]\n",
    "                    \n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path, image, label in zip(paths, images, labels.cpu().numpy().tolist()):\n",
    "                        if path in estimationerror_sample_path_list:\n",
    "                            start_time=time.time()      \n",
    "                            game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped_dict[backbone_type],\n",
    "                                                                         image)\n",
    "                            \n",
    "                            explanation_kernelshap = shapley.ShapleyRegression(game, \n",
    "                                                                    batch_size=128, \n",
    "                                                                    detect_convergence=False,\n",
    "                                                                    paired_sampling=False,\n",
    "                                                                    n_samples=200000,\n",
    "                                                                    variance_batches=60,\n",
    "                                                                    return_all=True)                              \n",
    "                            \n",
    "                            path_list.append(path)\n",
    "                            explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                            label_list.append(label)\n",
    "                            \n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'kernelshapnopair', \n",
    "                                                     path_list=path_list, \n",
    "                                                     estimation_list=explanation_kernelshap_list, \n",
    "                                                     label_list=label_list,\n",
    "                                                     shape=None)  \n",
    "                    \n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "\n",
    "                    estimationerror_save_dict_path=f'results/9_estimationerror/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(estimationerror_save_dict_path):\n",
    "                        try:\n",
    "                            with open(estimationerror_save_dict_path, 'rb') as f:\n",
    "                                estimationerror_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            estimationerror_save_dict_loaded={}\n",
    "                    else:\n",
    "                        estimationerror_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(estimationerror_save_dict_backbone_method)            \n",
    "                    len_loaded=len(estimationerror_save_dict_loaded)\n",
    "                    estimationerror_save_dict_backbone_method.update(estimationerror_save_dict_loaded)\n",
    "                    len_updated=len(estimationerror_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "                    \n",
    "                    with open(estimationerror_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(estimationerror_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass                    \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c14574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for path, data in estimationerror_save_dict[backbone_type][\"kernelshap\"].items():\n",
    "        explanation_save_dict[backbone_type]['kernelshap'][path]={\"explanation\": data['estimation'][0].values.T,\n",
    "                                                                  \"elapsed_time\": np.nan}\n",
    "        print(path, data['estimation'][0].values.T.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "        print(backbone_type, explanation_method, len(estimationerror_save_dict_backbone_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1639263",
   "metadata": {},
   "outputs": [],
   "source": [
    "[elapsed_time/len(paths) for i in range(len(paths))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4297a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsedtime_save_dict_update??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da58468",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_masked_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f6928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f61e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c839705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e3403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b2d9d0",
   "metadata": {},
   "source": [
    "# sensitivity-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6d44f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_players=196\n",
    "for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "    if dataset_split==\"test\":\n",
    "        if idx>int(1000/data_loader.batch_size+0.5):\n",
    "            break\n",
    "    if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "        continue                                \n",
    "\n",
    "    images=batch['images']\n",
    "    labels=batch['labels']\n",
    "    paths=batch['path']\n",
    "\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):            \n",
    "        for image, path, label in zip(images, paths, labels):\n",
    "            #path=path.replace('l0.cs.washington.edu','l2lambda.cs.washington.edu')\n",
    "            for num_included_players in [\"all\"]:\n",
    "                print(num_included_players)                \n",
    "                if num_included_players==\"all\":\n",
    "                    prob_all=[]\n",
    "                    mask_all=[]\n",
    "                    for random_iter in range(20):\n",
    "                        image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                        mask=generate_mask(num_players=num_players,\n",
    "                                           num_mask_samples=50,\n",
    "                                           paired_mask_samples=False,\n",
    "                                           mode=\"uniform\",\n",
    "                                           random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "                        surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                        with torch.no_grad():\n",
    "                            output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                  masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                        else:\n",
    "                            prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                        prob_all.append(prob)\n",
    "                        mask_all.append(mask)\n",
    "                    prob_all=np.concatenate(prob_all, axis=0)\n",
    "                    mask_all=np.concatenate(mask_all, axis=0)\n",
    "\n",
    "                for explanation_method in explanation_method_to_run:                \n",
    "                    print(explanation_method)\n",
    "                    explanation=explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys())[0])]['explanation']\n",
    "                    explanation=explanation+np.random.RandomState(42).uniform(low=0, high=1e-40, size=explanation.shape)\n",
    "                    explanation_mask=explanation@(mask_all.T)\n",
    "\n",
    "                    if len(explanation.shape)==1:\n",
    "                        correlation = np.array([stats.spearmanr(explanation_mask, prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                        assert correlation.shape==(_config[\"output_dim\"],)\n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        #correlation=stats.spearmanr(np.concatenate([explanation_mask, prob_all.T], axis=0), axis=1).correlation\n",
    "                        correlation = np.array([stats.spearmanr(explanation_mask[i], prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                        assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                        \n",
    "                        fig=plt.figure(figsize=(20,5))\n",
    "                        ax=fig.add_subplot(121)\n",
    "                        ax.scatter(explanation_mask[label], prob_all[:,label])#\n",
    "                        ax.set_xlabel(\"sum_explanation\")\n",
    "                        ax.set_ylabel(\"model output\")\n",
    "                        ax.set_title(explanation_method)\n",
    "                        \n",
    "                        ax=fig.add_subplot(122)\n",
    "                        for i in range(prob_all.shape[1]):\n",
    "                            if i>3:\n",
    "                                break\n",
    "                            if i!=label:\n",
    "                                ax.scatter(explanation_mask[i], prob_all[:,i])#, s=4)#\n",
    "                        ax.set_xlabel(\"sum_explanation\")\n",
    "                        ax.set_ylabel(\"model output\")\n",
    "                        ax.set_title(explanation_method)                        \n",
    "                        \n",
    "                        plt.show()\n",
    "                        \n",
    "                    else:\n",
    "                        raise\n",
    "#                     sensitivity_save_dit_update(backbone_type, explanation_method,\n",
    "#                                                  num_included_players=num_included_players,\n",
    "#                                                  path_list=[path], sensitivity_list=[correlation],\n",
    "#                                                  shape=(_config[\"output_dim\"],))     \n",
    "            break\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b35df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.scatter(explanation_mask[label], prob_all[:,label])#\n",
    "ax.set_xlabel(\"sum_explanation\")\n",
    "ax.set_ylabel(\"model output\")\n",
    "ax.set_title(explanation_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd97c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3e1c7e",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a068a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "fig = plt.figure()\n",
    "ax_temp=fig.add_subplot()\n",
    "plt.clf()\n",
    "\n",
    "def visualize_result(x, values, pred=None, vmin_vmax='separate',\n",
    "                     image_labels=['normal','abnormal']*5,\n",
    "                     class_labels=['normal','abnormal']*5):\n",
    "    # colormap\n",
    "    from matplotlib import cm\n",
    "    from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "    color_num = 1000\n",
    "    img_mean = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis]\n",
    "    img_std = np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]    \n",
    "\n",
    "    if isinstance(vmin_vmax,tuple):\n",
    "        vmin, vmax= vmin_vmax\n",
    "        assert vmin < vmax\n",
    "        if vmin * vmax < 0:\n",
    "            ratio=vmax/(-vmin+vmax)\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0, 1, color_num))\n",
    "            newcolors[:int(color_num*(1-ratio))] = seismic(np.linspace(0, 0.5, int(color_num*(1-ratio))))\n",
    "            newcolors[-int(color_num*ratio)-1:] = seismic(np.linspace(0.5, 1, int(color_num*ratio)+1))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "        elif vmin > 0:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "        else:\n",
    "            raise\n",
    "    elif vmin_vmax==\"separate\":\n",
    "        #seismic = cm.get_cmap('seismic', color_num)\n",
    "        #newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "        #newcmp = ListedColormap(newcolors)        \n",
    "        if values.min()>0:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)            \n",
    "        else:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(values.shape[0], 1+values.shape[1], figsize=(2*(1+values.shape[1]), 2*(values.shape[0]+1)))\n",
    "\n",
    "    assert len(image_labels)==values.shape[0]==(len(image_labels) if pred is None else pred.shape[0])\n",
    "    assert len(class_labels)==values.shape[1]==(len(class_labels) if pred is None else pred.shape[1])\n",
    "    \n",
    "    for row in range(axes.shape[0]):\n",
    "        for col in range(axes.shape[1]):\n",
    "            if col==0: # Image\n",
    "                im = x[row].numpy() * img_std + img_mean # (C, H, W)\n",
    "                im = im.transpose(1, 2, 0).astype(float) # (H, W, C)\n",
    "                im = np.clip(im, a_min=0, a_max=1)\n",
    "\n",
    "                axes[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "                axes[row, 0].set_ylabel('{}'.format(image_labels[row]), fontsize=12)\n",
    "            else: # Explanation\n",
    "                values_select=values[row, col-1]\n",
    "                values_select_min, values_select_max=values_select.min(),values_select.max()\n",
    "\n",
    "                if vmin_vmax==\"separate\":\n",
    "                    if values.min()>0:\n",
    "                        axes[row, col].imshow(values_select, cmap=newcmp,\n",
    "                                              vmin=values_select_min,\n",
    "                                              vmax=values_select_max)\n",
    "                    else:\n",
    "                        axes[row, col].imshow(values_select, cmap=newcmp, \n",
    "                                              vmin=-max([abs(values_select_min),abs(values_select_max)]), \n",
    "                                              vmax=max([abs(values_select_min),abs(values_select_max)]))\n",
    "                else:\n",
    "                    axes[row, col].imshow(values_select, cmap=newcmp, vmin=vmin, vmax=vmax)\n",
    "\n",
    "                if pred is None:\n",
    "                    axes[row, col].set_xlabel('{:.2f}/{:.2f}'.format(values_select_min, values_select_max), fontsize=12)\n",
    "                else:\n",
    "                    axes[row, col].set_xlabel('{:.2f} {:.2f}/{:.2f}'.format(pred[row, col-1], values_select_min, values_select_max), fontsize=12)            \n",
    "\n",
    "                # Class labels\n",
    "                if row == 0:\n",
    "                    axes[row, col].set_title('{}'.format(class_labels[col-1]), fontsize=12)  \n",
    "\n",
    "            axes[row, col].set_xticks([])\n",
    "            axes[row, col].set_yticks([])                           \n",
    "\n",
    "    if vmin_vmax!=\"separate\":\n",
    "        fig = plt.figure(figsize=(5, 0.5))\n",
    "        ax=fig.add_subplot()        \n",
    "        \n",
    "        sns.heatmap([[0,0],[0,0]],\n",
    "                    ax=ax_temp,\n",
    "                    cmap=newcmp,\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                    xticklabels=True,\n",
    "                    linewidths=1,\n",
    "                    linecolor=np.array([220,220,220,256])/256,\n",
    "                    cbar_ax=ax,\n",
    "                    cbar_kws={'fraction':0.1, \"ticks\":np.linspace(vmin, vmax, 5), \"orientation\": \"horizontal\"},\n",
    "                    cbar=True,\n",
    "                    alpha=1,edgecolor='black')#,legend=None)\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig=plt.figure()\n",
    "        ax=fig.add_subplot()\n",
    "        ax.hist(values.flatten(),bins=np.linspace(vmin, vmax, 20))\n",
    "        ax.set_yscale('log')\n",
    "        print(values.flatten().min(), values.flatten().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b53e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_all_all=[]\n",
    "mask_all_all=[]\n",
    "for idx, image in enumerate(x):\n",
    "    print(idx)\n",
    "    prob_all=[]\n",
    "    mask_all=[]\n",
    "    for random_iter in range(20):\n",
    "        image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "        mask=generate_mask(num_players=num_players,\n",
    "                           num_mask_samples=50,\n",
    "                           paired_mask_samples=False,\n",
    "                           mode=\"uniform\",\n",
    "                           random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "        surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "        with torch.no_grad():\n",
    "            output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                  masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            prob=output['logits'].sigmoid().cpu().numpy()\n",
    "        else:\n",
    "            prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "        prob_all.append(prob)\n",
    "        mask_all.append(mask)\n",
    "    prob_all=np.concatenate(prob_all, axis=0)\n",
    "    mask_all=np.concatenate(mask_all, axis=0)\n",
    "    #print(prob_all.shape, mask_all.shape)\n",
    "    prob_all_all.append(prob_all)\n",
    "    mask_all_all.append(mask_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0270ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "values.reshape(values.shape[0], values.shape[1], 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15b98b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    #get classifier\n",
    "    classifier_dict[backbone_type].eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier_dict[backbone_type](x.to(next(classifier_dict[backbone_type].parameters()).device), output_attentions=False)\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            pred=output['logits'].detach().sigmoid().cpu().data.numpy()\n",
    "        else:\n",
    "            pred=output['logits'].detach().softmax().cpu().data.numpy()\n",
    "    del output    \n",
    "    \n",
    "    # Get explanation (modified_LRP)\n",
    "    values=np.concatenate([np.concatenate([get_lrp_module_explanation(backbone_type, image.squeeze(0), class_index=i, mode='transformer_attribution').cpu() for i in range(_config[\"output_dim\"])], axis=0)[np.newaxis,:] for image in x])\n",
    "    values=values.reshape(values.shape[0], values.shape[1], 14, 14)\n",
    "    \n",
    "    visualize_result(x, pred=pred, values=values,\n",
    "                     class_labels=y_labels[1:2] if _config[\"output_dim\"]==1 else y_labels,\n",
    "                     image_labels=y_labels,\n",
    "                     vmin_vmax=\"separate\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    values_lrp=values.reshape(values.shape[0], values.shape[1], 196)\n",
    "    \n",
    "    \n",
    "    explainer_dict[backbone_type].eval()\n",
    "    with torch.no_grad():\n",
    "        values=explainer_dict[backbone_type](x.to(next(explainer_dict[backbone_type].parameters()).device))    \n",
    "        values=values[0].reshape(-1, _config[\"output_dim\"], 14, 14).cpu().numpy()\n",
    "        \n",
    "    visualize_result(x, pred=pred, values=values,\n",
    "                     class_labels=y_labels[1:2] if _config[\"output_dim\"]==1 else y_labels,\n",
    "                     image_labels=y_labels,\n",
    "                     vmin_vmax=(-0.2, 0.2))    \n",
    "    \n",
    "    values_ours=values.reshape(values.shape[0], values.shape[1], 196)\n",
    "    \n",
    "    #visualize_result(x, pred=pred.repeat(10, axis=1), values=values.repeat(10, axis=1), separate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitshapley",
   "language": "python",
   "name": "vitshapley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
